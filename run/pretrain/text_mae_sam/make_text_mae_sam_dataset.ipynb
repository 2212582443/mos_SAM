{
       "cells": [
              {
                     "cell_type": "code",
                     "execution_count": 1,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "/data/projects/src/github.com/any35/MOS\n"
                                   ]
                            }
                     ],
                     "source": [
                            "import os, sys\n",
                            "\n",
                            "os.chdir(\"../../..\")\n",
                            "print(os.path.abspath(\"./\"))"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 8,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stderr",
                                   "output_type": "stream",
                                   "text": [
                                          "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
                                          "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                                          "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
                                   ]
                            },
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "torch.Size([1, 40, 768]) dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
                                   ]
                            }
                     ],
                     "source": [
                            "from transformers import BertTokenizer, BertModel\n",
                            "from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n",
                            "from transformers.tokenization_utils_base import BatchEncoding\n",
                            "\n",
                            "tokenizer: BertTokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", local_files_only=True)\n",
                            "bert: BertModel = BertModel.from_pretrained(\"bert-base-multilingual-cased\", local_files_only=True)\n",
                            "\n",
                            "token: BatchEncoding = tokenizer(\n",
                            "    \"please explan EAT\",\n",
                            "    padding=\"max_length\",\n",
                            "    max_length=40,\n",
                            "    return_tensors=\"pt\",\n",
                            ")\n",
                            "result: BaseModelOutputWithPoolingAndCrossAttentions = bert(**token)\n",
                            "print(result.last_hidden_state.shape, token.keys())"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 6,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "torch.Size([1, 3, 768])\n"
                                   ]
                            }
                     ],
                     "source": [
                            "from mos.models.sam.modeling_sam.embedding.cmri_cls2text_embedding import *\n",
                            "\n",
                            "print(get_cls_text_embedding(2).shape)"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 2,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "processed ACDC.pt...\t 18006 pairs\n",
                                          "processed CMRI-0.pt...\t 149 pairs\n",
                                          "processed CMRI-1.pt...\t 136 pairs\n",
                                          "processed CMRI-2.pt...\t 154 pairs\n",
                                          "processed CMRI-3.pt...\t 150 pairs\n",
                                          "processed CMRI-4.pt...\t 152 pairs\n",
                                          "processed CMRI-5.pt...\t 162 pairs\n",
                                          "processed CMRI-6.pt...\t 145 pairs\n",
                                          "processed CMRI-7.pt...\t 162 pairs\n",
                                          "processed CMRI-8.pt...\t 146 pairs\n",
                                          "processed CMRI-9.pt...\t 145 pairs\n",
                                          "processed CT_MR_2D_Dataset_DA.pt...\t 48528 pairs\n",
                                          "processed EMIDEC.pt...\t 2124 pairs\n",
                                          "processed HVSMR2016.pt...\t 5269 pairs\n",
                                          "processed LeftAtrialSegmentationChallenge2013.pt...\t 2963 pairs\n",
                                          "processed LeftAtrialSegmentationKaggle.pt...\t 1351 pairs\n",
                                          "processed MMWHS2017.pt...\t 185103 pairs\n",
                                          "processed MyoPS2020.pt...\t 1968 pairs\n",
                                          "processed VarDA.pt...\t 6107 pairs\n",
                                          "processed mnms.pt...\t 35452 pairs\n",
                                          "processed msd-seg.pt...\t 1351 pairs\n",
                                          "processed yorku_CardiacMRIDataset.pt...\t 15033 pairs\n",
                                          "total 113751 train images, 324756 pairs!\n",
                                          "cls text count: 6297\n"
                                   ]
                            }
                     ],
                     "source": [
                            "# make token dataset\n",
                            "# 数据集需要\n",
                            "# *.pt {\n",
                            "#   \"image\": torch.tensor, # [N, H, W]\n",
                            "#   \"mae_pair\": torch.tensor, # [N, 7] (bs, [src_image_index, token_index, target_image_index*5])\n",
                            "#   \"mae_pair_simple\": torch.tensor, # [N, 3] (bs, [src_image_index, token_index, target_image_index])\n",
                            "# }\n",
                            "# token.pt {\n",
                            "#   \"token_list\": torch.tensor, # [N, seq_len, hidden_size]\n",
                            "#   \"token_selector\": torch.tensor, # [N_l, 2] (bs, [offset, len])\n",
                            "# }\n",
                            "import torch, glob, json\n",
                            "from mos.models.sam.modeling_sam.embedding.cmri_cls2text_embedding import cls2index_key, make_cls_text_compose\n",
                            "from mos.models.sam.modeling_sam.embedding.text_embedding import text2tensor\n",
                            "\n",
                            "os.makedirs(\".cache/dataset/text-mae-sam-dataset/split\", exist_ok=True)\n",
                            "\n",
                            "exclude_files = set([\"CMRI.pt\"])\n",
                            "\n",
                            "file_list = glob.glob(\".cache/dataset/mmae-dataset/split-1.5mm-segment-only/*.pt\")\n",
                            "file_list.sort()\n",
                            "\n",
                            "token_index_dict = dict()\n",
                            "all_text_list = []\n",
                            "token_selector_list = []\n",
                            "\n",
                            "total_image_count = 0\n",
                            "total_pair_count = 0\n",
                            "\n",
                            "for file in file_list:\n",
                            "    file_name = file.split(\"/\")[-1]\n",
                            "    if file_name in exclude_files:\n",
                            "        continue\n",
                            "\n",
                            "    data = torch.load(file)\n",
                            "\n",
                            "    train_pair = []  # (bs,[src_index, token_index, target_index*5])\n",
                            "    mae_pair_simple_list = []  # (bs,[src_index, token_index, target_index])\n",
                            "\n",
                            "    image = data[\"image\"]\n",
                            "\n",
                            "    if \"all_merged_pair\" in data:\n",
                            "        all_merged_pair = data[\"all_merged_pair\"].tolist()\n",
                            "    else:\n",
                            "        pairs = data[\"pair\"].tolist()\n",
                            "        all_merged_pair = [\n",
                            "            [src_modality, src_index, target_modality, 30, 30, 30, 30, target_index, -1, -1, -1, -1]\n",
                            "            for src_modality, src_index, target_modality, target_index in pairs\n",
                            "        ]\n",
                            "\n",
                            "    for pair in data[\"pair\"].tolist():\n",
                            "        src_modality, src_index, target_modality, target_index = pair\n",
                            "\n",
                            "        key = cls2index_key([target_modality, 30, 30, 30, 30])\n",
                            "        if key in token_index_dict:\n",
                            "            token_index = token_index_dict[key]\n",
                            "        else:\n",
                            "            token_index = len(token_selector_list)\n",
                            "            token_index_dict[key] = token_index\n",
                            "            text_list = make_cls_text_compose([target_modality, 30, 30, 30, 30])\n",
                            "            token_selector_list.append([len(all_text_list), len(text_list)])\n",
                            "            all_text_list.extend(text_list)\n",
                            "\n",
                            "        mae_pair_simple_list.append([src_index, token_index, target_index])\n",
                            "\n",
                            "    for merged_pair in all_merged_pair:\n",
                            "        src_modality, src_index, attr1, attr2, attr3, attr4, attr5, idx1, idx2, idx3, idx4, idx5 = merged_pair\n",
                            "\n",
                            "        key = cls2index_key([attr1, attr2, attr3, attr4, attr5])\n",
                            "        if key in token_index_dict:\n",
                            "            token_index = token_index_dict[key]\n",
                            "        else:\n",
                            "            token_index = len(token_selector_list)\n",
                            "            token_index_dict[key] = token_index\n",
                            "            text_list = make_cls_text_compose([attr1, attr2, attr3, attr4, attr5])\n",
                            "            token_selector_list.append([len(all_text_list), len(text_list)])\n",
                            "            all_text_list.extend(text_list)\n",
                            "        train_pair.append([src_index, token_index, idx1, idx2, idx3, idx4, idx5])\n",
                            "\n",
                            "    torch.save(\n",
                            "        {\n",
                            "            \"image\": image,\n",
                            "            \"mae_pair\": torch.tensor(train_pair),\n",
                            "            \"mae_pair_simple\": torch.tensor(mae_pair_simple_list),\n",
                            "        },\n",
                            "        f\".cache/dataset/text-mae-sam-dataset/split/{file_name}\",\n",
                            "    )\n",
                            "    print(f\"processed {file_name}...\\t {len(train_pair)} pairs\")\n",
                            "    total_image_count += image.shape[0]\n",
                            "    total_pair_count += len(train_pair)\n",
                            "\n",
                            "# todo, print max seq_len\n",
                            "print(f\"total {total_image_count} train images, {total_pair_count} pairs!\")\n",
                            "\n",
                            "print(f\"cls text count: {len(all_text_list)}\")\n",
                            "# print(\"\\n\".join(all_text_list))\n",
                            "\n",
                            "json.dump(all_text_list, open(\".cache/dataset/text-mae-sam-dataset/cls_text.json\", \"w\"), indent=2)\n",
                            "\n",
                            "token_list = []\n",
                            "for txt in all_text_list:\n",
                            "    token_list.append(text2tensor(txt, 40).detach())\n",
                            "token_list = torch.cat(token_list, dim=0).float()\n",
                            "\n",
                            "token_selector_list = torch.tensor(token_selector_list, dtype=torch.int32)\n",
                            "\n",
                            "assert len(token_selector_list.shape) == 2\n",
                            "assert len(token_list.shape) == 3 and token_list.shape[1:] == (40, 768)\n",
                            "\n",
                            "torch.save(\n",
                            "    {\n",
                            "        \"token_list\": token_list,\n",
                            "        \"token_selector\": token_selector_list,\n",
                            "    },\n",
                            "    \".cache/dataset/text-mae-sam-dataset/split/token.pt\",\n",
                            ")"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 3,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "processed dataset-0.pt...\n",
                                          "processed dataset-1.pt...\n",
                                          "processed dataset-2.pt...\n",
                                          "processed dataset-3.pt...\n",
                                          "processed dataset-4.pt...\n",
                                          "processed dataset-5.pt...\n",
                                          "processed dataset-6.pt...\n",
                                          "processed dataset-7.pt...\n",
                                          "processed dataset-8.pt...\n",
                                          "processed dataset-9.pt...\n"
                                   ]
                            }
                     ],
                     "source": [
                            "# 组合text-mae-sam-dataset数据集\n",
                            "\n",
                            "import torch, glob\n",
                            "\n",
                            "exclude_files = set([\"CMRI.pt\", \"token.pt\"])\n",
                            "for i in range(10):\n",
                            "    exclude_files.add(f\"CMRI-{i}.pt\")\n",
                            "\n",
                            "\n",
                            "file_list = glob.glob(\".cache/dataset/text-mae-sam-dataset/split/*.pt\")\n",
                            "file_list.sort()\n",
                            "\n",
                            "image_list = [\n",
                            "    torch.zeros(1, 214, 214, dtype=torch.uint8),  # zero padding\n",
                            "]\n",
                            "train_pair = []\n",
                            "mae_pair_simple_list = []\n",
                            "\n",
                            "label = 1\n",
                            "for file in file_list:\n",
                            "    file_name = file.split(\"/\")[-1]\n",
                            "    if file_name in exclude_files:\n",
                            "        continue\n",
                            "\n",
                            "    data = torch.load(file)\n",
                            "    image, mea_pair, mea_pair_simple = data[\"image\"], data[\"mae_pair\"], data[\"mae_pair_simple\"]\n",
                            "    #  (bs, [src_image_index, token_index, target_image_index*5])\n",
                            "    offset = mea_pair.clone()\n",
                            "    offset[offset >= 0] = label\n",
                            "    offset[offset == -1] = 1\n",
                            "    offset[:, 1] = 0\n",
                            "    mea_pair += offset\n",
                            "    train_pair.append(mea_pair)\n",
                            "\n",
                            "    # (bs, [src_image_index, token_index, target_image_index])\n",
                            "    offset = mea_pair_simple.clone()\n",
                            "    offset[offset >= 0] = label\n",
                            "    offset[offset == -1] = 1\n",
                            "    offset[:, 1] = 0\n",
                            "    mea_pair_simple += offset\n",
                            "    mae_pair_simple_list.append(mea_pair_simple)\n",
                            "\n",
                            "    image_list.append(image)\n",
                            "    label += image.shape[0]\n",
                            "\n",
                            "image_list = torch.cat(image_list, dim=0).to(torch.uint8)\n",
                            "train_pair = torch.cat(train_pair, dim=0)\n",
                            "mae_pair_simple_list = torch.cat(mae_pair_simple_list, dim=0)\n",
                            "\n",
                            "# load token\n",
                            "data = torch.load(\".cache/dataset/text-mae-sam-dataset/split/token.pt\")\n",
                            "token_list, token_selector_list = data[\"token_list\"], data[\"token_selector\"]\n",
                            "partition_index = label\n",
                            "\n",
                            "# 划分数据集, 做交叉测试\n",
                            "for partition in range(10):\n",
                            "    train_image_list = [image_list]\n",
                            "    train_pair_list = [train_pair]\n",
                            "    train_pair_simple_list = [mae_pair_simple_list]\n",
                            "\n",
                            "    train_cmri_only_list = []\n",
                            "\n",
                            "    image_index = label\n",
                            "\n",
                            "    valid_pair_list = []\n",
                            "    valid_pair_simple_list = []\n",
                            "    for i in range(10):\n",
                            "        file = f\".cache/dataset/text-mae-sam-dataset/split/CMRI-{i}.pt\"\n",
                            "        data = torch.load(file)\n",
                            "        image, mea_pair, mea_pair_simple = data[\"image\"], data[\"mae_pair\"], data[\"mae_pair_simple\"]\n",
                            "\n",
                            "        is_valid = i == partition\n",
                            "\n",
                            "        #  (bs, [src_image_index, token_index, target_image_index*5])\n",
                            "        offset = mea_pair.clone()\n",
                            "        offset[offset >= 0] = image_index\n",
                            "        offset[:, 1] = 0\n",
                            "        offset[offset == -1] = 1\n",
                            "        mea_pair += offset\n",
                            "        if is_valid:\n",
                            "            valid_pair_list.append(mea_pair)\n",
                            "        else:\n",
                            "            for _ in range(10):  # cmri数据扩充10倍\n",
                            "                train_pair_list.append(mea_pair)\n",
                            "\n",
                            "        # (bs, [src_image_index, token_index, target_image_index])\n",
                            "        offset = mea_pair_simple.clone()\n",
                            "        offset[offset >= 0] = image_index\n",
                            "        offset[:, 1] = 0\n",
                            "        offset[offset == -1] = 1\n",
                            "        mea_pair_simple += offset\n",
                            "        if is_valid:\n",
                            "            valid_pair_simple_list.append(mea_pair_simple)\n",
                            "        else:\n",
                            "            train_cmri_only_list.append(mea_pair_simple)\n",
                            "            for _ in range(10):  # cmri数据扩充10倍\n",
                            "                train_pair_simple_list.append(mea_pair_simple)\n",
                            "\n",
                            "        train_image_list.append(image)\n",
                            "        image_index += image.shape[0]\n",
                            "\n",
                            "    train_image_list = torch.cat(train_image_list, dim=0)\n",
                            "    train_pair_list = torch.cat(train_pair_list, dim=0)\n",
                            "    train_pair_simple_list = torch.cat(train_pair_simple_list, dim=0)\n",
                            "    train_cmri_only_list = torch.cat(train_cmri_only_list, dim=0)\n",
                            "    valid_pair_list = torch.cat(valid_pair_list, dim=0)\n",
                            "    valid_pair_simple_list = torch.cat(valid_pair_simple_list, dim=0)\n",
                            "\n",
                            "    torch.save(\n",
                            "        {\n",
                            "            \"image\": train_image_list.to(torch.uint8),\n",
                            "            \"token_list\": token_list.float(),\n",
                            "            \"token_selector_list\": token_selector_list.int(),\n",
                            "            \"train_mae_pair\": train_pair_list.int(),\n",
                            "            \"train_mae_pair_simple\": train_pair_simple_list.int(),\n",
                            "            \"train_cmri_pair_simple\": train_cmri_only_list.int(),\n",
                            "            \"valid_mae_pair\": valid_pair_list.int(),\n",
                            "            \"valid_mae_pair_simple\": valid_pair_simple_list.int(),\n",
                            "        },\n",
                            "        f\".cache/dataset/text-mae-sam-dataset/dataset-all-label-{partition}.pt\",\n",
                            "    )\n",
                            "    print(f\"processed dataset-all-label-{partition}.pt...\")"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 2,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          "processed ACDC.pt...\t 18006 pairs\n",
                                          "processed CMRI-0.pt...\t 149 pairs\n",
                                          "processed CMRI-1.pt...\t 136 pairs\n",
                                          "processed CMRI-2.pt...\t 154 pairs\n",
                                          "processed CMRI-3.pt...\t 150 pairs\n",
                                          "processed CMRI-4.pt...\t 152 pairs\n",
                                          "processed CMRI-5.pt...\t 162 pairs\n",
                                          "processed CMRI-6.pt...\t 145 pairs\n",
                                          "processed CMRI-7.pt...\t 162 pairs\n",
                                          "processed CMRI-8.pt...\t 146 pairs\n",
                                          "processed CMRI-9.pt...\t 145 pairs\n",
                                          "processed CT_MR_2D_Dataset_DA.pt...\t 20594 pairs\n",
                                          "processed EMIDEC.pt...\t 2124 pairs\n",
                                          "processed HVSMR2016.pt...\t 3158 pairs\n",
                                          "processed MMWHS2017.pt...\t 52218 pairs\n",
                                          "processed MyoPS2020.pt...\t 1968 pairs\n",
                                          "processed VarDA.pt...\t 6107 pairs\n",
                                          "processed mnms.pt...\t 35452 pairs\n",
                                          "processed yorku_CardiacMRIDataset.pt...\t 15033 pairs\n",
                                          "processed CMRI-0-image.pt...\t image 3090 pairs\n",
                                          "processed CMRI-1-image.pt...\t image 2340 pairs\n",
                                          "processed CMRI-2-image.pt...\t image 3000 pairs\n",
                                          "processed CMRI-3-image.pt...\t image 3210 pairs\n",
                                          "processed CMRI-4-image.pt...\t image 2970 pairs\n",
                                          "processed CMRI-5-image.pt...\t image 2850 pairs\n",
                                          "processed CMRI-6-image.pt...\t image 2970 pairs\n",
                                          "processed CMRI-7-image.pt...\t image 3120 pairs\n",
                                          "processed CMRI-8-image.pt...\t image 3070 pairs\n",
                                          "processed CMRI-9-image.pt...\t image 2830 pairs\n",
                                          "processed CMRI-private-image.pt...\t image 49662 pairs\n",
                                          "total 153479 train images, 321524 pairs!\n",
                                          "cls text count: 6221\n"
                                   ]
                            }
                     ],
                     "source": [
                            "# make token dataset with label filter (只保留和脂肪相关/附近的label, 并且加入unlable的图像)\n",
                            "# 数据集需要\n",
                            "# *.pt {\n",
                            "#   \"image\": torch.tensor, # [N, H, W]\n",
                            "#   \"mae_pair\": torch.tensor, # [N, 7] (bs, [src_image_index, token_index, target_image_index*5])\n",
                            "#   \"mae_pair_simple\": torch.tensor, # [N, 3] (bs, [src_image_index, token_index, target_image_index])\n",
                            "# }\n",
                            "# token.pt {\n",
                            "#   \"token_list\": torch.tensor, # [N, seq_len, hidden_size]\n",
                            "#   \"token_selector\": torch.tensor, # [N_l, 2] (bs, [offset, len])\n",
                            "# }\n",
                            "import torch, glob, json\n",
                            "from mos.models.sam.modeling_sam.embedding.cmri_cls2text_embedding import cls2index_key, make_cls_text_compose\n",
                            "from mos.models.sam.modeling_sam.embedding.text_embedding import text2tensor\n",
                            "\n",
                            "export_dir = \".cache/dataset/text-mae-sam-dataset/split-filter-label\"\n",
                            "\n",
                            "os.makedirs(export_dir, exist_ok=True)\n",
                            "\n",
                            "exclude_files = set([\"CMRI.pt\"])\n",
                            "\n",
                            "file_list = glob.glob(\".cache/dataset/mmae-dataset/split-1.5mm-segment-only/*.pt\")\n",
                            "file_list.sort()\n",
                            "\n",
                            "include_label = set([11, 12, 13, 20])  # LV,RV,MYO,EAT\n",
                            "include_image_type = set([0, 1, 2, 3, 4, 5, 7])  # 排除CT和超声影像\n",
                            "\n",
                            "\n",
                            "def is_any_included(label_list):\n",
                            "    for label in label_list:\n",
                            "        if label in include_label:\n",
                            "            return True\n",
                            "    return False\n",
                            "\n",
                            "\n",
                            "token_index_dict = dict()\n",
                            "all_text_list = []\n",
                            "token_selector_list = []\n",
                            "\n",
                            "total_image_count = 0\n",
                            "total_pair_count = 0\n",
                            "\n",
                            "for file in file_list:\n",
                            "    file_name = file.split(\"/\")[-1]\n",
                            "    if file_name in exclude_files:\n",
                            "        continue\n",
                            "\n",
                            "    data = torch.load(file)\n",
                            "\n",
                            "    new_image_index = []\n",
                            "    new_image_mapping = dict()\n",
                            "\n",
                            "    train_pair = []  # (bs,[src_index, token_index, target_index*5])\n",
                            "    mae_pair_simple_list = []  # (bs,[src_index, token_index, target_index])\n",
                            "\n",
                            "    image = data[\"image\"]\n",
                            "\n",
                            "    if \"all_merged_pair\" in data:\n",
                            "        all_merged_pair = data[\"all_merged_pair\"].tolist()\n",
                            "    else:\n",
                            "        pairs = data[\"pair\"].tolist()\n",
                            "        all_merged_pair = [\n",
                            "            [src_modality, src_index, target_modality, 30, 30, 30, 30, target_index, -1, -1, -1, -1]\n",
                            "            for src_modality, src_index, target_modality, target_index in pairs\n",
                            "        ]\n",
                            "\n",
                            "    for pair in data[\"pair\"].tolist():\n",
                            "        src_modality, src_index, target_modality, target_index = pair\n",
                            "\n",
                            "        if src_modality not in include_image_type:\n",
                            "            continue\n",
                            "\n",
                            "        if target_modality not in include_label:\n",
                            "            continue\n",
                            "\n",
                            "        if src_index not in new_image_mapping:\n",
                            "            new_image_mapping[src_index] = len(new_image_index)\n",
                            "            new_image_index.append(src_index)\n",
                            "        src_index = new_image_mapping[src_index]\n",
                            "\n",
                            "        if target_index not in new_image_mapping:\n",
                            "            new_image_mapping[target_index] = len(new_image_index)\n",
                            "            new_image_index.append(target_index)\n",
                            "        target_index = new_image_mapping[target_index]\n",
                            "\n",
                            "        key = cls2index_key([target_modality, 30, 30, 30, 30])\n",
                            "        if key in token_index_dict:\n",
                            "            token_index = token_index_dict[key]\n",
                            "        else:\n",
                            "            token_index = len(token_selector_list)\n",
                            "            token_index_dict[key] = token_index\n",
                            "            text_list = make_cls_text_compose([target_modality, 30, 30, 30, 30])\n",
                            "            token_selector_list.append([len(all_text_list), len(text_list)])\n",
                            "            all_text_list.extend(text_list)\n",
                            "\n",
                            "        mae_pair_simple_list.append([src_index, token_index, target_index])\n",
                            "\n",
                            "    for merged_pair in all_merged_pair:\n",
                            "        src_modality, src_index, attr1, attr2, attr3, attr4, attr5, idx1, idx2, idx3, idx4, idx5 = merged_pair\n",
                            "\n",
                            "        if src_modality not in include_image_type:\n",
                            "            continue\n",
                            "\n",
                            "        index_list = [idx1, idx2, idx3, idx4, idx5]\n",
                            "        target_modality_list = [attr1, attr2, attr3, attr4, attr5]\n",
                            "        if not is_any_included(target_modality_list):\n",
                            "            continue\n",
                            "\n",
                            "        for i, (label, index) in enumerate(zip(target_modality_list, index_list)):\n",
                            "            if index < 0:\n",
                            "                continue\n",
                            "            if index not in new_image_mapping:\n",
                            "                new_image_mapping[index] = len(new_image_index)\n",
                            "                new_image_index.append(index)\n",
                            "            index_list[i] = new_image_mapping[index]\n",
                            "\n",
                            "        key = cls2index_key([attr1, attr2, attr3, attr4, attr5])\n",
                            "        if key in token_index_dict:\n",
                            "            token_index = token_index_dict[key]\n",
                            "        else:\n",
                            "            token_index = len(token_selector_list)\n",
                            "            token_index_dict[key] = token_index\n",
                            "            text_list = make_cls_text_compose([attr1, attr2, attr3, attr4, attr5])\n",
                            "            token_selector_list.append([len(all_text_list), len(text_list)])\n",
                            "            all_text_list.extend(text_list)\n",
                            "        train_pair.append([src_index, token_index, *index_list])\n",
                            "\n",
                            "    if len(new_image_index) == 0:\n",
                            "        continue\n",
                            "    image = image[new_image_index]\n",
                            "\n",
                            "    torch.save(\n",
                            "        {\n",
                            "            \"image\": image,\n",
                            "            \"mae_pair\": torch.tensor(train_pair),\n",
                            "            \"mae_pair_simple\": torch.tensor(mae_pair_simple_list),\n",
                            "        },\n",
                            "        f\"{export_dir}/{file_name}\",\n",
                            "    )\n",
                            "    print(f\"processed {file_name}...\\t {len(train_pair)} pairs\")\n",
                            "    total_image_count += image.shape[0]\n",
                            "    total_pair_count += len(train_pair)\n",
                            "\n",
                            "# 加入CMRI图像\n",
                            "exclude_files = set([\"CMRI-private.pt\"])\n",
                            "file_list = glob.glob(\".cache/dataset/mmae-dataset/split-1.5mm-image-only/CMRI-*.pt\")\n",
                            "file_list.sort()\n",
                            "for file in file_list:\n",
                            "    file_name = file.split(\"/\")[-1]\n",
                            "    if file_name in exclude_files:\n",
                            "        continue\n",
                            "\n",
                            "    data = torch.load(file)\n",
                            "\n",
                            "    new_image_index = []\n",
                            "    new_image_mapping = dict()\n",
                            "\n",
                            "    image_pair_simple_list = []  # (bs,[src_index, token_index, target_index])\n",
                            "\n",
                            "    image = data[\"image\"]\n",
                            "\n",
                            "    pairs = data[\"pair\"].tolist()\n",
                            "    all_merged_pair = [\n",
                            "        [src_modality, src_index, target_modality, 30, 30, 30, 30, target_index, -1, -1, -1, -1]\n",
                            "        for src_modality, src_index, target_modality, target_index in pairs\n",
                            "    ]\n",
                            "\n",
                            "    for pair in data[\"pair\"].tolist():\n",
                            "        src_modality, src_index, target_modality, target_index = pair\n",
                            "\n",
                            "        if src_modality not in include_image_type:\n",
                            "            continue\n",
                            "\n",
                            "        if src_index not in new_image_mapping:\n",
                            "            new_image_mapping[src_index] = len(new_image_index)\n",
                            "            new_image_index.append(src_index)\n",
                            "        src_index = new_image_mapping[src_index]\n",
                            "\n",
                            "        if target_index not in new_image_mapping:\n",
                            "            new_image_mapping[target_index] = len(new_image_index)\n",
                            "            new_image_index.append(target_index)\n",
                            "        target_index = new_image_mapping[target_index]\n",
                            "\n",
                            "        key = cls2index_key([target_modality, 30, 30, 30, 30])\n",
                            "        if key in token_index_dict:\n",
                            "            token_index = token_index_dict[key]\n",
                            "        else:\n",
                            "            token_index = len(token_selector_list)\n",
                            "            token_index_dict[key] = token_index\n",
                            "            text_list = make_cls_text_compose([target_modality, 30, 30, 30, 30])\n",
                            "            token_selector_list.append([len(all_text_list), len(text_list)])\n",
                            "            all_text_list.extend(text_list)\n",
                            "\n",
                            "        image_pair_simple_list.append([src_index, token_index, target_index])\n",
                            "\n",
                            "    if len(new_image_index) == 0:\n",
                            "        continue\n",
                            "\n",
                            "    image = image[new_image_index]\n",
                            "\n",
                            "    file_name = file_name.replace(\".pt\", \"-image.pt\")\n",
                            "\n",
                            "    torch.save(\n",
                            "        {\n",
                            "            \"image\": image,\n",
                            "            \"image_pair_simple\": torch.tensor(image_pair_simple_list),\n",
                            "        },\n",
                            "        f\"{export_dir}/{file_name}\",\n",
                            "    )\n",
                            "    print(f\"processed {file_name}...\\t image {len(image_pair_simple_list)} pairs\")\n",
                            "    total_image_count += image.shape[0]\n",
                            "    total_pair_count += len(train_pair)\n",
                            "\n",
                            "# todo, print max seq_len\n",
                            "print(f\"total {total_image_count} train images, {total_pair_count} pairs!\")\n",
                            "\n",
                            "print(f\"cls text count: {len(all_text_list)}\")\n",
                            "# print(\"\\n\".join(all_text_list))\n",
                            "\n",
                            "json.dump(all_text_list, open(\".cache/dataset/text-mae-sam-dataset/cls_text-filter-label.json\", \"w\"), indent=2)\n",
                            "\n",
                            "token_list = []\n",
                            "for txt in all_text_list:\n",
                            "    token_list.append(text2tensor(txt, 40).detach())\n",
                            "token_list = torch.cat(token_list, dim=0).float()\n",
                            "\n",
                            "token_selector_list = torch.tensor(token_selector_list, dtype=torch.int32)\n",
                            "\n",
                            "assert len(token_selector_list.shape) == 2\n",
                            "assert len(token_list.shape) == 3 and token_list.shape[1:] == (40, 768)\n",
                            "\n",
                            "torch.save(\n",
                            "    {\n",
                            "        \"token_list\": token_list,\n",
                            "        \"token_selector\": token_selector_list,\n",
                            "    },\n",
                            "    f\"{export_dir}/token.pt\",\n",
                            ")"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 2,
                     "metadata": {},
                     "outputs": [
                            {
                                   "name": "stdout",
                                   "output_type": "stream",
                                   "text": [
                                          ".cache/dataset/text-mae-sam-dataset/split-filter-label/ACDC.pt\n",
                                          ".cache/dataset/text-mae-sam-dataset/split-filter-label/CT_MR_2D_Dataset_DA.pt\n",
                                          ".cache/dataset/text-mae-sam-dataset/split-filter-label/EMIDEC.pt\n",
                                          ".cache/dataset/text-mae-sam-dataset/split-filter-label/HVSMR2016.pt\n",
                                          ".cache/dataset/text-mae-sam-dataset/split-filter-label/MMWHS2017.pt\n",
                                          ".cache/dataset/text-mae-sam-dataset/split-filter-label/MyoPS2020.pt\n",
                                          ".cache/dataset/text-mae-sam-dataset/split-filter-label/VarDA.pt\n",
                                          ".cache/dataset/text-mae-sam-dataset/split-filter-label/mnms.pt\n",
                                          ".cache/dataset/text-mae-sam-dataset/split-filter-label/yorku_CardiacMRIDataset.pt\n",
                                          "processed dataset-filter-label-0.pt...\n",
                                          "processed dataset-filter-label-1.pt...\n",
                                          "processed dataset-filter-label-2.pt...\n",
                                          "processed dataset-filter-label-3.pt...\n",
                                          "processed dataset-filter-label-4.pt...\n",
                                          "processed dataset-filter-label-5.pt...\n",
                                          "processed dataset-filter-label-6.pt...\n",
                                          "processed dataset-filter-label-7.pt...\n",
                                          "processed dataset-filter-label-8.pt...\n",
                                          "processed dataset-filter-label-9.pt...\n"
                                   ]
                            }
                     ],
                     "source": [
                            "# 组合text-mae-sam-dataset数据集(只保留和脂肪相关/附近的label)\n",
                            "\n",
                            "import torch, glob\n",
                            "\n",
                            "exclude_files = set([\"CMRI.pt\", \"token.pt\"])\n",
                            "for i in range(10):\n",
                            "    exclude_files.add(f\"CMRI-{i}.pt\")\n",
                            "    exclude_files.add(f\"CMRI-{i}-image.pt\")\n",
                            "\n",
                            "src_dir = \".cache/dataset/text-mae-sam-dataset/split-filter-label\"\n",
                            "export_dir = \".cache/dataset/text-mae-sam-dataset\"\n",
                            "\n",
                            "file_list = glob.glob(f\"{src_dir}/*.pt\")\n",
                            "file_list.sort()\n",
                            "\n",
                            "image_list = [\n",
                            "    torch.zeros(1, 214, 214, dtype=torch.uint8),  # zero padding\n",
                            "]\n",
                            "train_pair = []\n",
                            "mae_pair_simple_list = []\n",
                            "\n",
                            "label = 1\n",
                            "for file in file_list:\n",
                            "    file_name = file.split(\"/\")[-1]\n",
                            "    if file_name in exclude_files:\n",
                            "        continue\n",
                            "\n",
                            "    print(file)\n",
                            "    data = torch.load(file)\n",
                            "    image, mea_pair, mea_pair_simple = data[\"image\"], data[\"mae_pair\"], data[\"mae_pair_simple\"]\n",
                            "    #  (bs, [src_image_index, token_index, target_image_index*5])\n",
                            "    offset = mea_pair.clone()\n",
                            "    offset[offset >= 0] = label\n",
                            "    offset[offset == -1] = 1\n",
                            "    offset[:, 1] = 0\n",
                            "    mea_pair += offset\n",
                            "    train_pair.append(mea_pair)\n",
                            "\n",
                            "    # (bs, [src_image_index, token_index, target_image_index])\n",
                            "    offset = mea_pair_simple.clone()\n",
                            "    offset[offset >= 0] = label\n",
                            "    offset[offset == -1] = 1\n",
                            "    offset[:, 1] = 0\n",
                            "    mea_pair_simple += offset\n",
                            "    mae_pair_simple_list.append(mea_pair_simple)\n",
                            "\n",
                            "    image_list.append(image)\n",
                            "    label += image.shape[0]\n",
                            "\n",
                            "image_list = torch.cat(image_list, dim=0).to(torch.uint8)\n",
                            "train_pair = torch.cat(train_pair, dim=0)\n",
                            "mae_pair_simple_list = torch.cat(mae_pair_simple_list, dim=0)\n",
                            "\n",
                            "# load token\n",
                            "data = torch.load(f\"{src_dir}/token.pt\")\n",
                            "token_list, token_selector_list = data[\"token_list\"], data[\"token_selector\"]\n",
                            "partition_index = label\n",
                            "\n",
                            "# 划分数据集, 做交叉测试\n",
                            "for partition in range(10):\n",
                            "    train_image_list = [image_list]\n",
                            "    train_pair_list = [train_pair]\n",
                            "    train_pair_simple_list = [mae_pair_simple_list]\n",
                            "\n",
                            "    train_cmri_only_list = []\n",
                            "\n",
                            "    image_index = label\n",
                            "\n",
                            "    valid_pair_list = []\n",
                            "    valid_pair_simple_list = []\n",
                            "    for i in range(10):\n",
                            "        file = f\"{src_dir}/CMRI-{i}.pt\"\n",
                            "        data = torch.load(file)\n",
                            "        image, mea_pair, mea_pair_simple = data[\"image\"], data[\"mae_pair\"], data[\"mae_pair_simple\"]\n",
                            "\n",
                            "        is_valid = i == partition\n",
                            "\n",
                            "        #  (bs, [src_image_index, token_index, target_image_index*5])\n",
                            "        offset = mea_pair.clone()\n",
                            "        offset[offset >= 0] = image_index\n",
                            "        offset[:, 1] = 0\n",
                            "        offset[offset == -1] = 1\n",
                            "        mea_pair += offset\n",
                            "        if is_valid:\n",
                            "            valid_pair_list.append(mea_pair)\n",
                            "        else:\n",
                            "            for _ in range(10):  # cmri数据扩充10倍\n",
                            "                train_pair_list.append(mea_pair)\n",
                            "\n",
                            "        # (bs, [src_image_index, token_index, target_image_index])\n",
                            "        offset = mea_pair_simple.clone()\n",
                            "        offset[offset >= 0] = image_index\n",
                            "        offset[:, 1] = 0\n",
                            "        offset[offset == -1] = 1\n",
                            "        mea_pair_simple += offset\n",
                            "        if is_valid:\n",
                            "            valid_pair_simple_list.append(mea_pair_simple)\n",
                            "        else:\n",
                            "            for _ in range(15):  # cmri数据扩充15倍\n",
                            "                train_cmri_only_list.append(mea_pair_simple)\n",
                            "            for _ in range(10):  # cmri数据扩充10倍\n",
                            "                train_pair_simple_list.append(mea_pair_simple)\n",
                            "\n",
                            "        train_image_list.append(image)\n",
                            "        image_index += image.shape[0]\n",
                            "\n",
                            "    for i in range(10):\n",
                            "        file = f\"{src_dir}/CMRI-{i}-image.pt\"\n",
                            "        data = torch.load(file)\n",
                            "        image, mea_pair_simple = data[\"image\"], data[\"image_pair_simple\"]\n",
                            "\n",
                            "        is_valid = i == partition\n",
                            "\n",
                            "        # (bs, [src_image_index, token_index, target_image_index])\n",
                            "        offset = mea_pair_simple.clone()\n",
                            "        offset[offset >= 0] = image_index\n",
                            "        offset[:, 1] = 0\n",
                            "        offset[offset == -1] = 1\n",
                            "        mea_pair_simple += offset\n",
                            "        if is_valid:\n",
                            "            # valid_pair_simple_list.append(mea_pair_simple)\n",
                            "            pass\n",
                            "        else:\n",
                            "            train_cmri_only_list.append(mea_pair_simple)\n",
                            "            train_pair_simple_list.append(mea_pair_simple)\n",
                            "\n",
                            "        train_image_list.append(image)\n",
                            "        image_index += image.shape[0]\n",
                            "\n",
                            "    train_image_list = torch.cat(train_image_list, dim=0)\n",
                            "    train_pair_list = torch.cat(train_pair_list, dim=0)\n",
                            "    train_pair_simple_list = torch.cat(train_pair_simple_list, dim=0)\n",
                            "    train_cmri_only_list = torch.cat(train_cmri_only_list, dim=0)\n",
                            "    valid_pair_list = torch.cat(valid_pair_list, dim=0)\n",
                            "    valid_pair_simple_list = torch.cat(valid_pair_simple_list, dim=0)\n",
                            "\n",
                            "    torch.save(\n",
                            "        {\n",
                            "            \"image\": train_image_list.to(torch.uint8),\n",
                            "            \"token_list\": token_list.float(),\n",
                            "            \"token_selector_list\": token_selector_list.int(),\n",
                            "            \"train_mae_pair\": train_pair_list.int(),\n",
                            "            \"train_mae_pair_simple\": train_pair_simple_list.int(),\n",
                            "            \"train_cmri_pair_simple\": train_cmri_only_list.int(),\n",
                            "            \"valid_mae_pair\": valid_pair_list.int(),\n",
                            "            \"valid_mae_pair_simple\": valid_pair_simple_list.int(),\n",
                            "        },\n",
                            "        f\"{export_dir}/dataset-filter-label-{partition}.pt\",\n",
                            "    )\n",
                            "    print(f\"processed dataset-filter-label-{partition}.pt...\")"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 3,
                     "metadata": {},
                     "outputs": [
                            {
                                   "data": {
                                          "application/vnd.jupyter.widget-view+json": {
                                                 "model_id": "21737daddd7747198eda7914bbee4237",
                                                 "version_major": 2,
                                                 "version_minor": 0
                                          },
                                          "text/plain": [
                                                 "interactive(children=(IntSlider(value=84089, description='pair_index', max=168179), Output()), _dom_classes=('…"
                                          ]
                                   },
                                   "metadata": {},
                                   "output_type": "display_data"
                            },
                            {
                                   "data": {
                                          "application/vnd.jupyter.widget-view+json": {
                                                 "model_id": "7ebd65e8a49c4f1a9ee615ee560ce5a8",
                                                 "version_major": 2,
                                                 "version_minor": 0
                                          },
                                          "text/plain": [
                                                 "interactive(children=(IntSlider(value=74, description='pair_index', max=148), Output()), _dom_classes=('widget…"
                                          ]
                                   },
                                   "metadata": {},
                                   "output_type": "display_data"
                            },
                            {
                                   "data": {
                                          "application/vnd.jupyter.widget-view+json": {
                                                 "model_id": "b6cf3272fc96468f9abeb5d3c93621e2",
                                                 "version_major": 2,
                                                 "version_minor": 0
                                          },
                                          "text/plain": [
                                                 "interactive(children=(IntSlider(value=43415, description='pair_index', max=86831), Output()), _dom_classes=('w…"
                                          ]
                                   },
                                   "metadata": {},
                                   "output_type": "display_data"
                            },
                            {
                                   "data": {
                                          "application/vnd.jupyter.widget-view+json": {
                                                 "model_id": "65f22414eb2f49af9a865b513dc4a2cd",
                                                 "version_major": 2,
                                                 "version_minor": 0
                                          },
                                          "text/plain": [
                                                 "interactive(children=(IntSlider(value=13855, description='pair_index', max=27711), Output()), _dom_classes=('w…"
                                          ]
                                   },
                                   "metadata": {},
                                   "output_type": "display_data"
                            },
                            {
                                   "data": {
                                          "application/vnd.jupyter.widget-view+json": {
                                                 "model_id": "6670cc80e3c44d44a509b8e5b4c65d01",
                                                 "version_major": 2,
                                                 "version_minor": 0
                                          },
                                          "text/plain": [
                                                 "interactive(children=(IntSlider(value=74, description='pair_index', max=148), Output()), _dom_classes=('widget…"
                                          ]
                                   },
                                   "metadata": {},
                                   "output_type": "display_data"
                            },
                            {
                                   "data": {
                                          "text/plain": [
                                                 "(<function __main__.draw_image.<locals>._draw_image(pair_index=(0, 168179))>,\n",
                                                 " <function __main__.draw_image.<locals>._draw_image(pair_index=(0, 148))>,\n",
                                                 " <function __main__.draw_image_simple.<locals>._draw_image(pair_index=(0, 86831))>,\n",
                                                 " <function __main__.draw_image_simple.<locals>._draw_image(pair_index=(0, 27711))>,\n",
                                                 " <function __main__.draw_image_simple.<locals>._draw_image(pair_index=(0, 148))>)"
                                          ]
                                   },
                                   "execution_count": 3,
                                   "metadata": {},
                                   "output_type": "execute_result"
                            }
                     ],
                     "source": [
                            "# show dataset\n",
                            "import torch\n",
                            "import matplotlib.pyplot as plt\n",
                            "from ipywidgets import interact\n",
                            "\n",
                            "file = \".cache/dataset/text-mae-sam-dataset/dataset-filter-label-0.pt\"\n",
                            "data = torch.load(file)\n",
                            "(\n",
                            "    all_image,  # (bs, h, w)\n",
                            "    train_mae_pair,  # (bs, [src_image_index, token_index, target_image_index*5])\n",
                            "    valid_mae_pair,\n",
                            "    train_mae_pair_simple,  # (bs, [src_image_index, token_index, target_image_index])\n",
                            "    train_cmri_pair_simple,\n",
                            "    valid_mae_pair_simple,\n",
                            ") = (\n",
                            "    data[\"image\"],\n",
                            "    data[\"train_mae_pair\"],\n",
                            "    data[\"valid_mae_pair\"],\n",
                            "    data[\"train_mae_pair_simple\"],\n",
                            "    data[\"train_cmri_pair_simple\"],\n",
                            "    data[\"valid_mae_pair_simple\"],\n",
                            ")\n",
                            "\n",
                            "\n",
                            "def draw_image(pair):\n",
                            "    pair_count = pair.shape[0] - 1\n",
                            "\n",
                            "    @interact\n",
                            "    def _draw_image(pair_index=(0, pair_count)):\n",
                            "        src_image_index, token_index, idx1, idx2, idx3, idx4, idx5 = pair[pair_index].tolist()\n",
                            "\n",
                            "        image = all_image[[src_image_index, idx1, idx2, idx3, idx4, idx5], ::]\n",
                            "\n",
                            "        image = image.permute(1, 0, 2).reshape(214, -1)\n",
                            "\n",
                            "        plt.title([src_image_index, token_index, idx1, idx2, idx3, idx4, idx5])\n",
                            "        plt.imshow(image, cmap=\"gray\")\n",
                            "\n",
                            "    return _draw_image\n",
                            "\n",
                            "\n",
                            "def draw_image_simple(pair):\n",
                            "    pair_count = pair.shape[0] - 1\n",
                            "\n",
                            "    @interact\n",
                            "    def _draw_image(pair_index=(0, pair_count)):\n",
                            "        src_image_index, token_index, target_index = pair[pair_index].tolist()\n",
                            "\n",
                            "        image = all_image[[src_image_index, target_index], ::]\n",
                            "\n",
                            "        image = image.permute(1, 0, 2).reshape(214, -1)\n",
                            "\n",
                            "        plt.title([src_image_index, token_index, target_index])\n",
                            "        plt.imshow(image, cmap=\"gray\")\n",
                            "\n",
                            "    return _draw_image\n",
                            "\n",
                            "\n",
                            "draw_image(train_mae_pair), draw_image(valid_mae_pair), draw_image_simple(train_mae_pair_simple), draw_image_simple(\n",
                            "    train_cmri_pair_simple\n",
                            "), draw_image_simple(valid_mae_pair_simple),"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 16,
                     "metadata": {},
                     "outputs": [
                            {
                                   "data": {
                                          "text/plain": [
                                                 "'.cache/dataset/text-mae-sam-dataset/split-filter-label/LeftAtrialSegmentationChallenge2013.pt'"
                                          ]
                                   },
                                   "execution_count": 16,
                                   "metadata": {},
                                   "output_type": "execute_result"
                            }
                     ],
                     "source": [
                            "file"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 2,
                     "metadata": {},
                     "outputs": [],
                     "source": [
                            "# 从sam-dataset迁移数据\n",
                            "import torch, json\n",
                            "from typing import Any\n",
                            "from mos.models.sam.modeling_sam.embedding.cmri_cls2text_embedding import cls2index_key, make_cls_text_compose\n",
                            "from mos.models.sam.modeling_sam.embedding.text_embedding import text2tensor\n",
                            "\n",
                            "\n",
                            "base_path = \".cache/dataset/sam-dataset\"\n",
                            "device = \"cpu\"\n",
                            "\n",
                            "valid_meta: list[dict[str, Any]] = json.load(open(f\"{base_path}/valid-metas.json\"))\n",
                            "valid_images = torch.load(f\"{base_path}/valid-images.ot\", map_location=device).squeeze(1) * 255  # (bs, h, w)\n",
                            "valid_segments = torch.load(f\"{base_path}/valid-segments.ot\", map_location=device) * 255  # (bs, h, w)\n",
                            "\n",
                            "train_meta: list[dict[str, Any]] = json.load(open(f\"{base_path}/train-metas.json\"))\n",
                            "train_images: torch.Tensor = torch.load(f\"{base_path}/train-images.ot\", map_location=device).squeeze(1) * 255\n",
                            "train_segments = torch.load(f\"{base_path}/train-segments.ot\", map_location=device) * 255\n",
                            "\n",
                            "# 1: EAT => 20\n",
                            "# 2: MYO => 13\n",
                            "# 3: RV => 12\n",
                            "train_image_types = [d[\"cls\"] for d in train_meta]\n",
                            "train_image_types = torch.tensor(train_image_types, dtype=torch.int32)\n",
                            "train_image_types[train_image_types == 1] = 20\n",
                            "train_image_types[train_image_types == 2] = 13\n",
                            "train_image_types[train_image_types == 3] = 12\n",
                            "\n",
                            "valid_image_types = [d[\"cls\"] for d in valid_meta]\n",
                            "valid_image_types = torch.tensor(valid_image_types, dtype=torch.int32)\n",
                            "valid_image_types[valid_image_types == 1] = 20\n",
                            "valid_image_types[valid_image_types == 2] = 13\n",
                            "valid_image_types[valid_image_types == 3] = 12\n",
                            "\n",
                            "image_list = []\n",
                            "train_pair = []\n",
                            "valid_pair = []\n",
                            "\n",
                            "token_index_dict = dict()\n",
                            "all_text_list = []\n",
                            "token_selector_list = []\n",
                            "\n",
                            "for i, (image, segment, target_modality) in enumerate(\n",
                            "    zip(train_images.split(1, 0), train_segments.split(1, 0), train_image_types.tolist())\n",
                            "):\n",
                            "    if segment.sum() <= 0:\n",
                            "        continue\n",
                            "\n",
                            "    key = cls2index_key([target_modality, 30, 30, 30, 30])\n",
                            "    if key in token_index_dict:\n",
                            "        token_index = token_index_dict[key]\n",
                            "    else:\n",
                            "        token_index = len(token_selector_list)\n",
                            "        token_index_dict[key] = token_index\n",
                            "        text_list = make_cls_text_compose([target_modality, 30, 30, 30, 30])\n",
                            "        token_selector_list.append([len(all_text_list), len(text_list)])\n",
                            "        all_text_list.extend(text_list)\n",
                            "\n",
                            "    src_index = len(image_list)\n",
                            "    target_index = src_index + 1\n",
                            "    image_list.append(image)\n",
                            "    image_list.append(segment)\n",
                            "    train_pair.append([src_index, token_index, target_index])\n",
                            "\n",
                            "for i, (image, segment, target_modality) in enumerate(\n",
                            "    zip(valid_images.split(1, 0), valid_segments.split(1, 0), valid_image_types.tolist())\n",
                            "):\n",
                            "    if segment.sum() <= 0:\n",
                            "        continue\n",
                            "\n",
                            "    key = cls2index_key([target_modality, 30, 30, 30, 30])\n",
                            "    if key in token_index_dict:\n",
                            "        token_index = token_index_dict[key]\n",
                            "    else:\n",
                            "        token_index = len(token_selector_list)\n",
                            "        token_index_dict[key] = token_index\n",
                            "        text_list = make_cls_text_compose([target_modality, 30, 30, 30, 30])\n",
                            "        token_selector_list.append([len(all_text_list), len(text_list)])\n",
                            "        all_text_list.extend(text_list)\n",
                            "\n",
                            "    src_index = len(image_list)\n",
                            "    target_index = src_index + 1\n",
                            "    image_list.append(image)\n",
                            "    image_list.append(segment)\n",
                            "    valid_pair.append([src_index, token_index, target_index])\n",
                            "\n",
                            "train_pair = torch.tensor(train_pair, dtype=torch.int32)\n",
                            "valid_pair = torch.tensor(valid_pair, dtype=torch.int32)\n",
                            "image_list = torch.cat(image_list, dim=0)\n",
                            "\n",
                            "token_list = []\n",
                            "for txt in all_text_list:\n",
                            "    token_list.append(text2tensor(txt, 40).detach())\n",
                            "token_list = torch.cat(token_list, dim=0).float()\n",
                            "token_selector_list = torch.tensor(token_selector_list, dtype=torch.int32)\n",
                            "assert len(token_selector_list.shape) == 2\n",
                            "assert len(token_list.shape) == 3 and token_list.shape[1:] == (40, 768)\n",
                            "\n",
                            "json.dump(all_text_list, open(\".cache/dataset/text-mae-sam-dataset/dataset-reprod-0.json\", \"w\"), indent=2)\n",
                            "\n",
                            "data = {\n",
                            "    \"image\": image_list,\n",
                            "    \"train_mae_pair\": torch.empty(0),\n",
                            "    \"train_mae_pair_simple\": torch.empty(0),\n",
                            "    \"train_cmri_pair_simple\": train_pair,\n",
                            "    \"valid_mae_pair\": torch.empty(0),\n",
                            "    \"valid_mae_pair_simple\": valid_pair,\n",
                            "    \"token_list\": token_list,\n",
                            "    \"token_selector_list\": token_selector_list,\n",
                            "}\n",
                            "torch.save(\n",
                            "    data,\n",
                            "    \".cache/dataset/text-mae-sam-dataset/dataset-reprod-0.pt\",\n",
                            ")"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 19,
                     "metadata": {},
                     "outputs": [
                            {
                                   "data": {
                                          "application/vnd.jupyter.widget-view+json": {
                                                 "model_id": "9b2cbb9c8866463fb7874a4c968389bc",
                                                 "version_major": 2,
                                                 "version_minor": 0
                                          },
                                          "text/plain": [
                                                 "interactive(children=(Checkbox(value=True, description='show_segment'), IntSlider(value=5810, description='sli…"
                                          ]
                                   },
                                   "metadata": {},
                                   "output_type": "display_data"
                            },
                            {
                                   "data": {
                                          "text/plain": [
                                                 "<function __main__.ui_draw_segment_3d.<locals>._draw_segment_ui_3d(show_segment=True, slice_index=(0, 11620))>"
                                          ]
                                   },
                                   "execution_count": 19,
                                   "metadata": {},
                                   "output_type": "execute_result"
                            }
                     ],
                     "source": [
                            "def plot_segment(imgae, segment, alpha=0.4):\n",
                            "    import matplotlib.pyplot as plt\n",
                            "\n",
                            "    \"\"\"Plot segment on image\n",
                            "    Args:\n",
                            "        imgae: [h, w]\n",
                            "        segment: [h, w]\n",
                            "    \"\"\"\n",
                            "    plt.imshow(imgae, cmap=\"gray\")\n",
                            "    plt.imshow(segment, cmap=\"jet\", alpha=alpha)\n",
                            "    plt.show()\n",
                            "\n",
                            "\n",
                            "def ui_draw_segment_3d(image, segment, alpha=0.4):\n",
                            "    from ipywidgets import interact\n",
                            "    import matplotlib.pyplot as plt\n",
                            "\n",
                            "    img_slices, _h, _w = image.shape\n",
                            "\n",
                            "    @interact\n",
                            "    def _draw_segment_ui_3d(show_segment=True, slice_index=(0, img_slices - 1)):\n",
                            "        if show_segment is None:\n",
                            "            return\n",
                            "        if show_segment and segment is not None:\n",
                            "            plot_segment(image[slice_index, ::], segment[slice_index, ::], alpha)\n",
                            "        else:\n",
                            "            plt.imshow(image[slice_index, ::], cmap=\"gray\")\n",
                            "\n",
                            "    return _draw_segment_ui_3d\n",
                            "\n",
                            "\n",
                            "# 1: EAT\n",
                            "# 2: MYO\n",
                            "# 3: RV\n",
                            "ui_draw_segment_3d(train_images, train_segments * (train_image_types == 3)[:, None, None])"
                     ]
              },
              {
                     "cell_type": "code",
                     "execution_count": 28,
                     "metadata": {},
                     "outputs": [
                            {
                                   "data": {
                                          "text/plain": [
                                                 "True"
                                          ]
                                   },
                                   "execution_count": 28,
                                   "metadata": {},
                                   "output_type": "execute_result"
                            }
                     ],
                     "source": []
              }
       ],
       "metadata": {
              "kernelspec": {
                     "display_name": "py39",
                     "language": "python",
                     "name": "python3"
              },
              "language_info": {
                     "codemirror_mode": {
                            "name": "ipython",
                            "version": 3
                     },
                     "file_extension": ".py",
                     "mimetype": "text/x-python",
                     "name": "python",
                     "nbconvert_exporter": "python",
                     "pygments_lexer": "ipython3",
                     "version": "3.10.11"
              }
       },
       "nbformat": 4,
       "nbformat_minor": 2
}
