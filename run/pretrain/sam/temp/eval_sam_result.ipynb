{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 把CMRI数据集按照 uid - sequence - slice_index 顺序加载\n",
    "\n",
    "    h x w = 224 x 224\n",
    "- 提取 dias 和 sys 两个时间点的图像, 以及target = 1的图像, 如果target = 1的图像已经有segment, 则随机提取另外一个target\n",
    "- 对dias, sys, target图像进行分割\n",
    "    +   根据dias, sys的segment确定分割的范围,\n",
    "    +   参考这个范围裁剪4个128x128的图像\n",
    "    +   每个图像单独进行分割\n",
    "    +   对分割结果进行合并, 最终生成合并后的segment\n",
    "    +   segment的值为2\n",
    "    +   对于dia,sys的分割而言, 重合的部分值为1, 本次多的部分为2, 本次少的部分为3\n",
    "- 把dias, sys, target 拼接为一个图像, 并记录图像的slice个数和uid, seqid到meta中\n",
    "- 所有user都进行这个操作, 最终把所有的图像/分割cat起来, 生成一个dcm数据\n",
    "- 保存dcm数据/meta数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model... ./.checkpoint/sammae-ck2020/latest\n",
      "load model.pt\n",
      "\tmask_decoder.iou_token.weight: torch.Size([1, 256])\n",
      "\tmask_decoder.mask_tokens.weight: torch.Size([4, 256])\n",
      "\tmask_decoder.transformer.layers.0.self_attn.q_proj.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.transformer.layers.0.self_attn.q_proj.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.0.self_attn.k_proj.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.transformer.layers.0.self_attn.k_proj.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.0.self_attn.v_proj.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.transformer.layers.0.self_attn.v_proj.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.0.self_attn.out_proj.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.transformer.layers.0.self_attn.out_proj.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.0.layer_norm1.weight: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.0.layer_norm1.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight: torch.Size([256, 128])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.0.layer_norm2.weight: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.0.layer_norm2.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.0.mlp.lin1.weight: torch.Size([2048, 256])\n",
      "\tmask_decoder.transformer.layers.0.mlp.lin1.bias: torch.Size([2048])\n",
      "\tmask_decoder.transformer.layers.0.mlp.lin2.weight: torch.Size([256, 2048])\n",
      "\tmask_decoder.transformer.layers.0.mlp.lin2.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.0.layer_norm3.weight: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.0.layer_norm3.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.0.layer_norm4.weight: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.0.layer_norm4.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight: torch.Size([256, 128])\n",
      "\tmask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.self_attn.q_proj.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.transformer.layers.1.self_attn.q_proj.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.self_attn.k_proj.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.transformer.layers.1.self_attn.k_proj.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.self_attn.v_proj.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.transformer.layers.1.self_attn.v_proj.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.self_attn.out_proj.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.transformer.layers.1.self_attn.out_proj.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.layer_norm1.weight: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.layer_norm1.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight: torch.Size([256, 128])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.layer_norm2.weight: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.layer_norm2.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.mlp.lin1.weight: torch.Size([2048, 256])\n",
      "\tmask_decoder.transformer.layers.1.mlp.lin1.bias: torch.Size([2048])\n",
      "\tmask_decoder.transformer.layers.1.mlp.lin2.weight: torch.Size([256, 2048])\n",
      "\tmask_decoder.transformer.layers.1.mlp.lin2.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.layer_norm3.weight: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.layer_norm3.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.layer_norm4.weight: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.layer_norm4.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight: torch.Size([256, 128])\n",
      "\tmask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.final_attn_token_to_image.q_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.final_attn_token_to_image.q_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.final_attn_token_to_image.k_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.final_attn_token_to_image.k_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.final_attn_token_to_image.v_proj.weight: torch.Size([128, 256])\n",
      "\tmask_decoder.transformer.final_attn_token_to_image.v_proj.bias: torch.Size([128])\n",
      "\tmask_decoder.transformer.final_attn_token_to_image.out_proj.weight: torch.Size([256, 128])\n",
      "\tmask_decoder.transformer.final_attn_token_to_image.out_proj.bias: torch.Size([256])\n",
      "\tmask_decoder.transformer.layer_norm_final_attn.weight: torch.Size([256])\n",
      "\tmask_decoder.transformer.layer_norm_final_attn.bias: torch.Size([256])\n",
      "\tmask_decoder.upscale_conv.0.weight: torch.Size([256, 128, 2, 2])\n",
      "\tmask_decoder.upscale_conv.0.bias: torch.Size([128])\n",
      "\tmask_decoder.upscale_conv.1.weight: torch.Size([128, 64, 2, 2])\n",
      "\tmask_decoder.upscale_conv.1.bias: torch.Size([64])\n",
      "\tmask_decoder.upscale_conv.2.weight: torch.Size([64, 32, 2, 2])\n",
      "\tmask_decoder.upscale_conv.2.bias: torch.Size([32])\n",
      "\tmask_decoder.upscale_layer_norm.0.weight: torch.Size([128])\n",
      "\tmask_decoder.upscale_layer_norm.0.bias: torch.Size([128])\n",
      "\tmask_decoder.upscale_layer_norm.1.weight: torch.Size([64])\n",
      "\tmask_decoder.upscale_layer_norm.1.bias: torch.Size([64])\n",
      "\tmask_decoder.output_hypernetworks_mlps.0.proj_in.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.0.proj_in.bias: torch.Size([256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.0.proj_out.weight: torch.Size([32, 256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.0.proj_out.bias: torch.Size([32])\n",
      "\tmask_decoder.output_hypernetworks_mlps.0.layers.0.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.0.layers.0.bias: torch.Size([256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.1.proj_in.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.1.proj_in.bias: torch.Size([256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.1.proj_out.weight: torch.Size([32, 256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.1.proj_out.bias: torch.Size([32])\n",
      "\tmask_decoder.output_hypernetworks_mlps.1.layers.0.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.1.layers.0.bias: torch.Size([256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.2.proj_in.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.2.proj_in.bias: torch.Size([256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.2.proj_out.weight: torch.Size([32, 256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.2.proj_out.bias: torch.Size([32])\n",
      "\tmask_decoder.output_hypernetworks_mlps.2.layers.0.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.2.layers.0.bias: torch.Size([256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.3.proj_in.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.3.proj_in.bias: torch.Size([256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.3.proj_out.weight: torch.Size([32, 256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.3.proj_out.bias: torch.Size([32])\n",
      "\tmask_decoder.output_hypernetworks_mlps.3.layers.0.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.output_hypernetworks_mlps.3.layers.0.bias: torch.Size([256])\n",
      "\tmask_decoder.iou_prediction_head.proj_in.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.iou_prediction_head.proj_in.bias: torch.Size([256])\n",
      "\tmask_decoder.iou_prediction_head.proj_out.weight: torch.Size([4, 256])\n",
      "\tmask_decoder.iou_prediction_head.proj_out.bias: torch.Size([4])\n",
      "\tmask_decoder.iou_prediction_head.layers.0.weight: torch.Size([256, 256])\n",
      "\tmask_decoder.iou_prediction_head.layers.0.bias: torch.Size([256])\n",
      "\tprompt.masked_img_embd: torch.Size([1, 256, 1, 1])\n",
      "\tprompt.embed_image.vision_encoder.vit.cls_token: torch.Size([1, 1, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.pos_embed: torch.Size([1, 257, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.patch_embed.proj.weight: torch.Size([768, 1, 8, 8])\n",
      "\tprompt.embed_image.vision_encoder.vit.patch_embed.proj.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.0.norm1.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.0.norm1.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.0.attn.qkv.weight: torch.Size([2304, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.0.attn.qkv.bias: torch.Size([2304])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.0.attn.proj.weight: torch.Size([768, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.0.attn.proj.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.0.norm2.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.0.norm2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.0.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.0.mlp.fc1.bias: torch.Size([3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.0.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.0.mlp.fc2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.1.norm1.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.1.norm1.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.1.attn.qkv.weight: torch.Size([2304, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.1.attn.qkv.bias: torch.Size([2304])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.1.attn.proj.weight: torch.Size([768, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.1.attn.proj.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.1.norm2.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.1.norm2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.1.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.1.mlp.fc1.bias: torch.Size([3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.1.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.1.mlp.fc2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.2.norm1.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.2.norm1.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.2.attn.qkv.weight: torch.Size([2304, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.2.attn.qkv.bias: torch.Size([2304])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.2.attn.proj.weight: torch.Size([768, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.2.attn.proj.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.2.norm2.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.2.norm2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.2.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.2.mlp.fc1.bias: torch.Size([3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.2.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.2.mlp.fc2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.3.norm1.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.3.norm1.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.3.attn.qkv.weight: torch.Size([2304, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.3.attn.qkv.bias: torch.Size([2304])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.3.attn.proj.weight: torch.Size([768, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.3.attn.proj.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.3.norm2.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.3.norm2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.3.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.3.mlp.fc1.bias: torch.Size([3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.3.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.3.mlp.fc2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.4.norm1.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.4.norm1.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.4.attn.qkv.weight: torch.Size([2304, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.4.attn.qkv.bias: torch.Size([2304])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.4.attn.proj.weight: torch.Size([768, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.4.attn.proj.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.4.norm2.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.4.norm2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.4.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.4.mlp.fc1.bias: torch.Size([3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.4.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.4.mlp.fc2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.5.norm1.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.5.norm1.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.5.attn.qkv.weight: torch.Size([2304, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.5.attn.qkv.bias: torch.Size([2304])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.5.attn.proj.weight: torch.Size([768, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.5.attn.proj.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.5.norm2.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.5.norm2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.5.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.5.mlp.fc1.bias: torch.Size([3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.5.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.5.mlp.fc2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.6.norm1.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.6.norm1.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.6.attn.qkv.weight: torch.Size([2304, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.6.attn.qkv.bias: torch.Size([2304])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.6.attn.proj.weight: torch.Size([768, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.6.attn.proj.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.6.norm2.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.6.norm2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.6.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.6.mlp.fc1.bias: torch.Size([3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.6.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.6.mlp.fc2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.7.norm1.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.7.norm1.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.7.attn.qkv.weight: torch.Size([2304, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.7.attn.qkv.bias: torch.Size([2304])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.7.attn.proj.weight: torch.Size([768, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.7.attn.proj.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.7.norm2.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.7.norm2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.7.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.7.mlp.fc1.bias: torch.Size([3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.7.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.7.mlp.fc2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.8.norm1.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.8.norm1.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.8.attn.qkv.weight: torch.Size([2304, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.8.attn.qkv.bias: torch.Size([2304])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.8.attn.proj.weight: torch.Size([768, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.8.attn.proj.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.8.norm2.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.8.norm2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.8.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.8.mlp.fc1.bias: torch.Size([3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.8.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.8.mlp.fc2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.9.norm1.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.9.norm1.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.9.attn.qkv.weight: torch.Size([2304, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.9.attn.qkv.bias: torch.Size([2304])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.9.attn.proj.weight: torch.Size([768, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.9.attn.proj.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.9.norm2.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.9.norm2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.9.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.9.mlp.fc1.bias: torch.Size([3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.9.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.9.mlp.fc2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.10.norm1.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.10.norm1.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.10.attn.qkv.weight: torch.Size([2304, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.10.attn.qkv.bias: torch.Size([2304])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.10.attn.proj.weight: torch.Size([768, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.10.attn.proj.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.10.norm2.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.10.norm2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.10.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.10.mlp.fc1.bias: torch.Size([3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.10.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.10.mlp.fc2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.11.norm1.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.11.norm1.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.11.attn.qkv.weight: torch.Size([2304, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.11.attn.qkv.bias: torch.Size([2304])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.11.attn.proj.weight: torch.Size([768, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.11.attn.proj.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.11.norm2.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.11.norm2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.11.mlp.fc1.weight: torch.Size([3072, 768])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.11.mlp.fc1.bias: torch.Size([3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.11.mlp.fc2.weight: torch.Size([768, 3072])\n",
      "\tprompt.embed_image.vision_encoder.vit.blocks.11.mlp.fc2.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.norm.weight: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.vit.norm.bias: torch.Size([768])\n",
      "\tprompt.embed_image.vision_encoder.neck.conv1.weight: torch.Size([256, 768, 1, 1])\n",
      "\tprompt.embed_image.vision_encoder.neck.layer_norm1.weight: torch.Size([256])\n",
      "\tprompt.embed_image.vision_encoder.neck.layer_norm1.bias: torch.Size([256])\n",
      "\tprompt.embed_image.vision_encoder.neck.conv2.weight: torch.Size([256, 256, 3, 3])\n",
      "\tprompt.embed_image.vision_encoder.neck.layer_norm2.weight: torch.Size([256])\n",
      "\tprompt.embed_image.vision_encoder.neck.layer_norm2.bias: torch.Size([256])\n",
      "\tprompt.embed_mask.conv1.weight: torch.Size([4, 1, 2, 2])\n",
      "\tprompt.embed_mask.conv1.bias: torch.Size([4])\n",
      "\tprompt.embed_mask.conv2.weight: torch.Size([16, 4, 2, 2])\n",
      "\tprompt.embed_mask.conv2.bias: torch.Size([16])\n",
      "\tprompt.embed_mask.conv3.weight: torch.Size([256, 16, 1, 1])\n",
      "\tprompt.embed_mask.conv3.bias: torch.Size([256])\n",
      "\tprompt.embed_mask.layer_norm1.weight: torch.Size([4])\n",
      "\tprompt.embed_mask.layer_norm1.bias: torch.Size([4])\n",
      "\tprompt.embed_mask.layer_norm2.weight: torch.Size([16])\n",
      "\tprompt.embed_mask.layer_norm2.bias: torch.Size([16])\n",
      "\tprompt.embed_mask.no_mask_embed.weight: torch.Size([1, 256])\n",
      "\tprompt.embed_point.object_point.weight: torch.Size([1, 256])\n",
      "\tprompt.embed_point.not_object_point.weight: torch.Size([1, 256])\n",
      "\tprompt.embed_point.background_point.weight: torch.Size([1, 256])\n",
      "\tprompt.embed_box.left_top_embedding.weight: torch.Size([1, 256])\n",
      "\tprompt.embed_box.right_bottom_embedding.weight: torch.Size([1, 256])\n",
      "\tprompt.embed_box.mlp.lin1.weight: torch.Size([1024, 512])\n",
      "\tprompt.embed_box.mlp.lin1.bias: torch.Size([1024])\n",
      "\tprompt.embed_box.mlp.lin2.weight: torch.Size([256, 1024])\n",
      "\tprompt.embed_box.mlp.lin2.bias: torch.Size([256])\n",
      "\tprompt.embed_text.proj_in.weight: torch.Size([256, 768])\n",
      "\tprompt.embed_text.proj_in.bias: torch.Size([256])\n",
      "\tprompt.embed_text.layers.0.weight: torch.Size([256, 256])\n",
      "\tprompt.embed_text.layers.0.bias: torch.Size([256])\n",
      "\tprompt.embed_text.proj_out.weight: torch.Size([256, 256])\n",
      "\tprompt.embed_text.proj_out.bias: torch.Size([256])\n",
      "\tprompt.constrcut_embedding.reconstruct_image_embedding: torch.Size([1, 1, 256])\n",
      "{'uid': 10, 'sys': 301, 'dias': 319}\n",
      "torch.Size([11, 1, 224, 224]) 71 199 56 184\n",
      "torch.Size([11, 1, 224, 224]) 49 177 56 184\n",
      "torch.Size([11, 1, 224, 224]) 71 199 29 157\n",
      "torch.Size([11, 1, 224, 224]) 49 177 29 157\n",
      "torch.Size([11, 1, 224, 224]) 60 188 42 170\n",
      "torch.Size([11, 1, 224, 224]) 80 208 62 190\n",
      "torch.Size([11, 1, 224, 224]) 39 167 62 190\n",
      "torch.Size([11, 1, 224, 224]) 80 208 25 153\n",
      "torch.Size([11, 1, 224, 224]) 39 167 25 153\n",
      "torch.Size([11, 1, 224, 224]) 59 187 43 171\n",
      "{'uid': 20, 'sys': 622, 'dias': 638}\n",
      "torch.Size([10, 1, 224, 224]) 60 188 43 171\n",
      "torch.Size([10, 1, 224, 224]) 8 136 43 171\n",
      "torch.Size([10, 1, 224, 224]) 60 188 0 128\n",
      "torch.Size([10, 1, 224, 224]) 8 136 0 128\n",
      "torch.Size([10, 1, 224, 224]) 34 162 18 146\n",
      "torch.Size([10, 1, 224, 224]) 66 194 46 174\n",
      "torch.Size([10, 1, 224, 224]) 4 132 46 174\n",
      "torch.Size([10, 1, 224, 224]) 66 194 0 128\n",
      "torch.Size([10, 1, 224, 224]) 4 132 0 128\n",
      "torch.Size([10, 1, 224, 224]) 35 163 20 148\n",
      "{'uid': 30, 'sys': 910, 'dias': 928}\n",
      "torch.Size([11, 1, 224, 224]) 67 195 51 179\n",
      "torch.Size([11, 1, 224, 224]) 11 139 51 179\n",
      "torch.Size([11, 1, 224, 224]) 67 195 16 144\n",
      "torch.Size([11, 1, 224, 224]) 11 139 16 144\n",
      "torch.Size([11, 1, 224, 224]) 39 167 33 161\n",
      "torch.Size([11, 1, 224, 224]) 74 202 55 183\n",
      "torch.Size([11, 1, 224, 224]) 9 137 55 183\n",
      "torch.Size([11, 1, 224, 224]) 74 202 10 138\n",
      "torch.Size([11, 1, 224, 224]) 9 137 10 138\n",
      "torch.Size([11, 1, 224, 224]) 41 169 32 160\n",
      "{'uid': 50, 'sys': 941, 'dias': 959}\n",
      "torch.Size([12, 1, 224, 224]) 54 182 59 187\n",
      "torch.Size([12, 1, 224, 224]) 14 142 59 187\n",
      "torch.Size([12, 1, 224, 224]) 54 182 28 156\n",
      "torch.Size([12, 1, 224, 224]) 14 142 28 156\n",
      "torch.Size([12, 1, 224, 224]) 34 162 43 171\n",
      "torch.Size([12, 1, 224, 224]) 59 187 58 186\n",
      "torch.Size([12, 1, 224, 224]) 13 141 58 186\n",
      "torch.Size([12, 1, 224, 224]) 59 187 26 154\n",
      "torch.Size([12, 1, 224, 224]) 13 141 26 154\n",
      "torch.Size([12, 1, 224, 224]) 36 164 42 170\n",
      "{'uid': 60, 'sys': 1241, 'dias': 1259}\n",
      "torch.Size([18, 1, 224, 224]) 60 188 62 190\n",
      "torch.Size([18, 1, 224, 224]) 25 153 62 190\n",
      "torch.Size([18, 1, 224, 224]) 60 188 27 155\n",
      "torch.Size([18, 1, 224, 224]) 25 153 27 155\n",
      "torch.Size([18, 1, 224, 224]) 42 170 44 172\n",
      "torch.Size([18, 1, 224, 224]) 62 190 66 194\n",
      "torch.Size([18, 1, 224, 224]) 24 152 66 194\n",
      "torch.Size([18, 1, 224, 224]) 62 190 24 152\n",
      "torch.Size([18, 1, 224, 224]) 24 152 24 152\n",
      "torch.Size([18, 1, 224, 224]) 43 171 45 173\n",
      "{'uid': 70, 'sys': 1511, 'dias': 1529}\n",
      "torch.Size([10, 1, 224, 224]) 70 198 48 176\n",
      "torch.Size([10, 1, 224, 224]) 35 163 48 176\n",
      "torch.Size([10, 1, 224, 224]) 70 198 20 148\n",
      "torch.Size([10, 1, 224, 224]) 35 163 20 148\n",
      "torch.Size([10, 1, 224, 224]) 52 180 34 162\n",
      "torch.Size([10, 1, 224, 224]) 69 197 55 183\n",
      "torch.Size([10, 1, 224, 224]) 24 152 55 183\n",
      "torch.Size([10, 1, 224, 224]) 69 197 15 143\n",
      "torch.Size([10, 1, 224, 224]) 24 152 15 143\n",
      "torch.Size([10, 1, 224, 224]) 46 174 35 163\n",
      "{'uid': 80, 'sys': 1811, 'dias': 1829}\n",
      "torch.Size([16, 1, 224, 224]) 67 195 42 170\n",
      "torch.Size([16, 1, 224, 224]) 27 155 42 170\n",
      "torch.Size([16, 1, 224, 224]) 67 195 22 150\n",
      "torch.Size([16, 1, 224, 224]) 27 155 22 150\n",
      "torch.Size([16, 1, 224, 224]) 47 175 32 160\n",
      "torch.Size([16, 1, 224, 224]) 68 196 43 171\n",
      "torch.Size([16, 1, 224, 224]) 24 152 43 171\n",
      "torch.Size([16, 1, 224, 224]) 68 196 26 154\n",
      "torch.Size([16, 1, 224, 224]) 24 152 26 154\n",
      "torch.Size([16, 1, 224, 224]) 46 174 34 162\n",
      "{'uid': 90, 'sys': 2111, 'dias': 2129}\n",
      "torch.Size([15, 1, 224, 224]) 79 207 56 184\n",
      "torch.Size([15, 1, 224, 224]) 67 195 56 184\n",
      "torch.Size([15, 1, 224, 224]) 79 207 17 145\n",
      "torch.Size([15, 1, 224, 224]) 67 195 17 145\n",
      "torch.Size([15, 1, 224, 224]) 73 201 36 164\n",
      "torch.Size([15, 1, 224, 224]) 82 210 59 187\n",
      "torch.Size([15, 1, 224, 224]) 50 178 59 187\n",
      "torch.Size([15, 1, 224, 224]) 82 210 17 145\n",
      "torch.Size([15, 1, 224, 224]) 50 178 17 145\n",
      "torch.Size([15, 1, 224, 224]) 66 194 38 166\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np, json\n",
    "from mos.models.sam.modeling_sam.sam_model import SamModel\n",
    "from mos.models.sam.modeling_sam.embedding.typing import DenseEmbeddingsTensor, TextTokenEmbeddingTensor\n",
    "from mos.utils.tensors import save_tensor_file\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from run.pretrain.sam.model_factory import ModelFactory\n",
    "from run.pretrain.sam.token_text import get_cls_text_embedding\n",
    "\n",
    "dataset_path = \"~/dataset/cmri\"\n",
    "dataset_path = os.path.expanduser(dataset_path)\n",
    "\n",
    "meta = os.path.join(dataset_path, \"dataset.json\")\n",
    "meta = json.load(open(meta, \"r\"))\n",
    "\n",
    "# dic[{uid, dias, sys, slice_count}]\n",
    "infos = {}\n",
    "\n",
    "files = meta[\"files\"]\n",
    "for index, file in enumerate(files):\n",
    "    uid = file[\"uid\"]\n",
    "    if uid % 10 != 0:\n",
    "        continue\n",
    "\n",
    "    if f\"{uid}\" not in infos:\n",
    "        infos[f\"{uid}\"] = {\"uid\": uid}\n",
    "    info = infos[f\"{uid}\"]\n",
    "\n",
    "    if file[\"diastole\"]:\n",
    "        info[\"dias\"] = index\n",
    "    if file[\"systole\"]:\n",
    "        info[\"sys\"] = index\n",
    "\n",
    "infos_list = list(infos.values())\n",
    "infos_list = sorted(infos_list, key=lambda x: x[\"uid\"])\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_factory = ModelFactory(device, \"sammae-ck2020\")\n",
    "model: SamModel = model_factory.load_model()\n",
    "\n",
    "IMGAE_SIZE = 224\n",
    "CROP_SIZE = 128\n",
    "\n",
    "\n",
    "def get_crop_box(mask_box) -> list[tuple[int, int, int, int]]:\n",
    "    \"\"\"\n",
    "    :param mask_box: x1, y1, x2, y2\n",
    "    :return: x1, y1, x2, y2\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = mask_box\n",
    "\n",
    "    def refine_box(x1, y1, x2, y2):\n",
    "        if x1 < 0:\n",
    "            x1 = 0\n",
    "            x2 = CROP_SIZE\n",
    "        if x2 > IMGAE_SIZE:\n",
    "            x2 = IMGAE_SIZE\n",
    "            x1 = IMGAE_SIZE - CROP_SIZE\n",
    "        if y1 < 0:\n",
    "            y1 = 0\n",
    "            y2 = CROP_SIZE\n",
    "        if y2 > IMGAE_SIZE:\n",
    "            y2 = IMGAE_SIZE\n",
    "            y1 = IMGAE_SIZE - CROP_SIZE\n",
    "        return x1, y1, x2, y2\n",
    "\n",
    "    return [\n",
    "        refine_box(x1, y1, x1 + CROP_SIZE, y1 + CROP_SIZE),\n",
    "        refine_box(x2 - CROP_SIZE, y1, x2, y1 + CROP_SIZE),\n",
    "        refine_box(x1, y2 - CROP_SIZE, x1 + CROP_SIZE, y2),\n",
    "        refine_box(x2 - CROP_SIZE, y2 - CROP_SIZE, x2, y2),\n",
    "        refine_box(\n",
    "            (x2 + x1 - CROP_SIZE) // 2,\n",
    "            (y2 + y1 - CROP_SIZE) // 2,\n",
    "            (x2 + x1 + CROP_SIZE) // 2,\n",
    "            (y2 + y1 + CROP_SIZE) // 2,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "def pred_segment(image: torch.Tensor, mask_box: tuple[int, int, int, int]):\n",
    "    prompt_text: TextTokenEmbeddingTensor = get_cls_text_embedding(1).to(device)\n",
    "    prompt_text = model.prompt.embed_text(prompt_text)\n",
    "    prompt_text = prompt_text.repeat(image.shape[0], 1, 1)  # bs, n, 768\n",
    "\n",
    "    dense_embeddings: DenseEmbeddingsTensor = model.prompt.embed_mask(None)\n",
    "\n",
    "    image = image.unsqueeze(1).to(device)  # bs, 1, h ,w\n",
    "\n",
    "    #  crop image\n",
    "    avg_segment = torch.zeros((image.shape[0], IMGAE_SIZE, IMGAE_SIZE)).to(device)\n",
    "    for x1, y1, x2, y2 in get_crop_box(mask_box):\n",
    "        print(image.shape, x1, x2, y1, y2)\n",
    "        crop_image = image[:, :, y1:y2, x1:x2]\n",
    "        crop_image = model.prompt.embed_image.vision_encoder(crop_image)\n",
    "        _, pred_masks = model(\n",
    "            crop_image,\n",
    "            prompt_text,\n",
    "            dense_embeddings,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        pred_masks = pred_masks[:, 0, 0, :, :]\n",
    "        pred_masks = pred_masks > 0.5\n",
    "        pred_masks = pred_masks.float()\n",
    "        avg_segment[:, y1:y2, x1:x2] += pred_masks\n",
    "    avg_segment = avg_segment >= 1\n",
    "    return avg_segment.to(torch.uint8)\n",
    "\n",
    "\n",
    "def fusion_pred_label_mask(pred_mask, label_mask):\n",
    "    result = torch.zeros_like(pred_mask).int()\n",
    "    result[pred_mask * label_mask == 1] = 1\n",
    "    result[pred_mask > label_mask] = 2\n",
    "    result[pred_mask < label_mask] = 3\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_get_segment_box(segment: torch.Tensor):\n",
    "    segment = segment.sum(dim=0, keepdim=True)\n",
    "    box = masks_to_boxes(segment).squeeze(0).tolist()\n",
    "    return [int(x) for x in box]\n",
    "\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    image_list = []\n",
    "    mask_list = []\n",
    "    for info in infos_list:\n",
    "        print(info)\n",
    "        dias = os.path.join(dataset_path, files[info[\"dias\"]][\"file_name\"])\n",
    "        dias = dict(np.load(dias))\n",
    "        dias_image, dias_segment = dias[\"image\"], dias[\"segment\"]\n",
    "        dias_image, dias_segment = torch.from_numpy(dias_image).to(device), torch.from_numpy(dias_segment).to(device)\n",
    "        dias_pred_mask = pred_segment(dias_image, get_get_segment_box(dias_segment))\n",
    "        dias_segment = fusion_pred_label_mask(dias_pred_mask, dias_segment)\n",
    "\n",
    "        sys = os.path.join(dataset_path, files[info[\"sys\"]][\"file_name\"])\n",
    "        sys = dict(np.load(sys))\n",
    "        sys_image, sys_segment = sys[\"image\"], sys[\"segment\"]\n",
    "        sys_image, sys_segment = torch.from_numpy(sys_image).to(device), torch.from_numpy(sys_segment).to(device)\n",
    "        sys_pred_mask = pred_segment(sys_image, get_get_segment_box(sys_segment))\n",
    "        sys_segment = fusion_pred_label_mask(sys_pred_mask, sys_segment)\n",
    "\n",
    "        # bs, h, w\n",
    "        mask = torch.cat(\n",
    "            [\n",
    "                dias_segment,\n",
    "                sys_segment,\n",
    "            ],\n",
    "            dim=2,\n",
    "        )\n",
    "        image = torch.cat(\n",
    "            [\n",
    "                dias_image,\n",
    "                sys_image,\n",
    "            ],\n",
    "            dim=2,\n",
    "        )\n",
    "        image = image.squeeze(1)\n",
    "\n",
    "        info[\"slice_count\"] = image.shape[0]\n",
    "        image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu()\n",
    "        mask = mask.permute(1, 2, 0).to(torch.uint8).cpu()\n",
    "        image_list.append(image)\n",
    "        mask_list.append(mask)\n",
    "\n",
    "    image = torch.cat(image_list, dim=2)\n",
    "    mask = torch.cat(mask_list, dim=2)\n",
    "    # save to dicom\n",
    "    save_tensor_file(image, \"/tmp/eval-sam-image.dcm\")\n",
    "    save_tensor_file(mask, \"/tmp/eval-sam-segment.dcm\")\n",
    "    # save info\n",
    "    json.dump(infos_list, open(\"/tmp/eval-sam-info.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f56be093340>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(mask.permute(2, 0, 1)[2, :, :], cmap=\"jet\", alpha=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
