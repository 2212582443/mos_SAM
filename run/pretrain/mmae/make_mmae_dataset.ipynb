{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/huping/home/projects/src/github.com/any35/MOS\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "os.chdir(\"../../..\")\n",
    "print(os.path.abspath(\"./\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed .cache/dataset/mmae-dataset/split/ACDC.pt...\t 46387 images, 46387 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/CMRI-private.pt...\t 48195 images, 49662 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/CMRI.pt...\t 30951 images, 30951 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/CT_MR_2D_Dataset_DA.pt...\t 9403 images, 9403 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/CoA_MRIData.pt...\t 2641 images, 2641 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/DOI_10.7910_DVN_CI3WB6.pt...\t 42225 images, 42225 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/DOI_10.7910_DVN_JMZHVI.pt...\t 16000 images, 16000 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/DOI_10.7910_DVN_N1R1Q4.pt...\t 1050 images, 1050 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/EMIDEC.pt...\t 2482 images, 2482 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/HVSMR2016.pt...\t 6382 images, 6382 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/LeftAtrialSegmentationChallenge2013.pt...\t 6031 images, 6031 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/LeftAtrialSegmentationKaggle.pt...\t 3622 images, 3622 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/MMWHS2017.pt...\t 50608 images, 50608 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/MyoPS2020.pt...\t 813 images, 2439 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/RegionalMulti-viewLearningForCardiacMotionAnalysis.pt...\t 4101 images, 4101 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/SunnybrookCardiacData.pt...\t 51545 images, 51545 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/VarDA.pt...\t 3856 images, 3856 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/mnms.pt...\t 122118 images, 122118 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/msd-seg.pt...\t 4919 images, 4919 pairs\n",
      "processed .cache/dataset/mmae-dataset/split/yorku_CardiacMRIDataset.pt...\t 18002 images, 18002 pairs\n",
      "471331 images & 474424 pairs saved!\n"
     ]
    }
   ],
   "source": [
    "# convert mmae-dataset to 1.5mm resolution\n",
    "import torch, glob, os\n",
    "from torchvision.transforms import functional as F, InterpolationMode\n",
    "\n",
    "os.makedirs(\".cache/dataset/mmae-dataset/split-1.5mm\", exist_ok=True)\n",
    "\n",
    "total_image_count = 0\n",
    "total_pair_count = 0\n",
    "file_list = glob.glob(\".cache/dataset/mmae-dataset/split/*.pt\")\n",
    "file_list.sort()\n",
    "\n",
    "for file in file_list:\n",
    "    data = torch.load(file)\n",
    "    image, pair = data[\"image\"], data[\"pair\"]\n",
    "\n",
    "    image = image.to(torch.uint8)\n",
    "    image = F.resize(\n",
    "        image,\n",
    "        (214, 214),\n",
    "        interpolation=InterpolationMode.BILINEAR,\n",
    "        antialias=True,\n",
    "    )\n",
    "    total_image_count += image.shape[0]\n",
    "    total_pair_count += pair.shape[0]\n",
    "    print(f\"processed {file}...\\t {image.shape[0]} images, { pair.shape[0]} pairs\")\n",
    "\n",
    "    data[\"image\"] = image\n",
    "    file_name = os.path.basename(file)\n",
    "\n",
    "    torch.save(\n",
    "        data,\n",
    "        f\".cache/dataset/mmae-dataset/split-1.5mm/{file_name}\",\n",
    "    )\n",
    "\n",
    "print(f\"{total_image_count} images & {total_pair_count} pairs saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed ACDC.pt...\t 10883 images, 8041 pairs\n",
      "processed CMRI-private.pt...\t 0 images, 0 pairs\n",
      "processed CMRI.pt...\t 3002 images, 1501 pairs\n",
      "processed CT_MR_2D_Dataset_DA.pt...\t 9403 images, 7835 pairs\n",
      "processed CoA_MRIData.pt...\t 0 images, 0 pairs\n",
      "processed DOI_10.7910_DVN_CI3WB6.pt...\t 0 images, 0 pairs\n",
      "processed DOI_10.7910_DVN_JMZHVI.pt...\t 0 images, 0 pairs\n",
      "processed DOI_10.7910_DVN_N1R1Q4.pt...\t 0 images, 0 pairs\n",
      "processed EMIDEC.pt...\t 2124 images, 1416 pairs\n",
      "processed HVSMR2016.pt...\t 4145 images, 2782 pairs\n",
      "processed LeftAtrialSegmentationChallenge2013.pt...\t 4426 images, 2463 pairs\n",
      "processed LeftAtrialSegmentationKaggle.pt...\t 2702 images, 1351 pairs\n",
      "processed MMWHS2017.pt...\t 33300 images, 26628 pairs\n",
      "processed MyoPS2020.pt...\t 597 images, 873 pairs\n",
      "processed RegionalMulti-viewLearningForCardiacMotionAnalysis.pt...\t 0 images, 0 pairs\n",
      "processed SunnybrookCardiacData.pt...\t 0 images, 0 pairs\n",
      "processed VarDA.pt...\t 3785 images, 2779 pairs\n",
      "processed mnms.pt...\t 21649 images, 15959 pairs\n",
      "processed msd-seg.pt...\t 2702 images, 1351 pairs\n",
      "processed yorku_CardiacMRIDataset.pt...\t 15033 images, 10022 pairs\n",
      "total 113751 train images, 83001 pairs!\n"
     ]
    }
   ],
   "source": [
    "# filter segment only & pair\n",
    "import torch, glob\n",
    "from torchvision.transforms import functional as F, InterpolationMode\n",
    "\n",
    "os.makedirs(\".cache/dataset/mmae-dataset/split-1.5mm-segment-only\", exist_ok=True)\n",
    "\n",
    "file_list = glob.glob(\".cache/dataset/mmae-dataset/split-1.5mm/*.pt\")\n",
    "file_list.sort()\n",
    "\n",
    "total_image_count = 0\n",
    "total_pair_count = 0\n",
    "\n",
    "\n",
    "def is_label(modality: int):\n",
    "    return modality >= 11 and modality <= 20\n",
    "\n",
    "\n",
    "def is_any_label(a: int, b: int, c: int, d: int, e: int):\n",
    "    return is_label(a) or is_label(b) or is_label(c) or is_label(d) or is_label(e)\n",
    "\n",
    "\n",
    "for file in file_list:\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    data = torch.load(file)\n",
    "\n",
    "    new_image_index = []\n",
    "    new_image_mapping = dict()\n",
    "\n",
    "    new_pair_list = []\n",
    "    image, pair = data[\"image\"], data[\"pair\"]\n",
    "    pair_list = pair.tolist()\n",
    "    for pair in pair.tolist():\n",
    "        src_modality, src_index, target_modality, target_index = pair\n",
    "        if not is_label(target_modality):\n",
    "            continue\n",
    "\n",
    "        if src_index not in new_image_mapping:\n",
    "            new_image_mapping[src_index] = len(new_image_index)\n",
    "            new_image_index.append(src_index)\n",
    "        if target_index not in new_image_mapping:\n",
    "            new_image_mapping[target_index] = len(new_image_index)\n",
    "            new_image_index.append(target_index)\n",
    "\n",
    "        src_index = new_image_mapping[src_index]\n",
    "        target_index = new_image_mapping[target_index]\n",
    "        new_pair_list.append([src_modality, src_index, target_modality, target_index])\n",
    "    data[\"pair\"] = torch.tensor(new_pair_list)\n",
    "\n",
    "    for merged_pair_key in [\"merged_pair\", \"all_merged_pair\"]:\n",
    "        if merged_pair_key not in data:\n",
    "            continue\n",
    "        new_merged_pair_list = []\n",
    "        for merged_pair in data[merged_pair_key].tolist():\n",
    "            src_modality, src_index, attr1, attr2, attr3, attr4, attr5, idx1, idx2, idx3, idx4, idx5 = merged_pair\n",
    "            if not is_any_label(attr1, attr2, attr3, attr4, attr5):\n",
    "                continue\n",
    "            if src_index not in new_image_mapping:\n",
    "                new_image_mapping[src_index] = len(new_image_index)\n",
    "                new_image_index.append(src_index)\n",
    "            src_index = new_image_mapping[src_index]\n",
    "            if is_label(attr1):\n",
    "                if idx1 not in new_image_mapping:\n",
    "                    new_image_mapping[idx1] = len(new_image_index)\n",
    "                    new_image_index.append(idx1)\n",
    "                idx1 = new_image_mapping[idx1]\n",
    "            if is_label(attr2):\n",
    "                if idx2 not in new_image_mapping:\n",
    "                    new_image_mapping[idx2] = len(new_image_index)\n",
    "                    new_image_index.append(idx2)\n",
    "                idx2 = new_image_mapping[idx2]\n",
    "            if is_label(attr3):\n",
    "                if idx3 not in new_image_mapping:\n",
    "                    new_image_mapping[idx3] = len(new_image_index)\n",
    "                    new_image_index.append(idx3)\n",
    "                idx3 = new_image_mapping[idx3]\n",
    "            if is_label(attr4):\n",
    "                if idx4 not in new_image_mapping:\n",
    "                    new_image_mapping[idx4] = len(new_image_index)\n",
    "                    new_image_index.append(idx4)\n",
    "                idx4 = new_image_mapping[idx4]\n",
    "            if is_label(attr5):\n",
    "                if idx5 not in new_image_mapping:\n",
    "                    new_image_mapping[idx5] = len(new_image_index)\n",
    "                    new_image_index.append(idx5)\n",
    "                idx5 = new_image_mapping[idx5]\n",
    "            new_merged_pair_list.append(\n",
    "                [\n",
    "                    src_modality,\n",
    "                    src_index,\n",
    "                    attr1,\n",
    "                    attr2,\n",
    "                    attr3,\n",
    "                    attr4,\n",
    "                    attr5,\n",
    "                    idx1,\n",
    "                    idx2,\n",
    "                    idx3,\n",
    "                    idx4,\n",
    "                    idx5,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        data[merged_pair_key] = torch.tensor(new_merged_pair_list)\n",
    "\n",
    "    data[\"image_meta\"] = data[\"image_meta\"][new_image_index]\n",
    "    data[\"image\"] = image[new_image_index]\n",
    "    if len(new_image_index) > 0:\n",
    "        torch.save(\n",
    "            data,\n",
    "            f\".cache/dataset/mmae-dataset/split-1.5mm-segment-only/{file_name}\",\n",
    "        )\n",
    "    print(f\"processed {file_name}...\\t {len(new_image_index)} images, {len(new_pair_list)} pairs\")\n",
    "    total_image_count += len(new_image_index)\n",
    "    total_pair_count += len(new_pair_list)\n",
    "\n",
    "print(f\"total {total_image_count} train images, {total_pair_count} pairs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition 0, 298 images, 149 pairs\n",
      "partition 1, 272 images, 136 pairs\n",
      "partition 2, 308 images, 154 pairs\n",
      "partition 3, 300 images, 150 pairs\n",
      "partition 4, 304 images, 152 pairs\n",
      "partition 5, 324 images, 162 pairs\n",
      "partition 6, 290 images, 145 pairs\n",
      "partition 7, 324 images, 162 pairs\n",
      "partition 8, 292 images, 146 pairs\n",
      "partition 9, 290 images, 145 pairs\n"
     ]
    }
   ],
   "source": [
    "# spit segment only CMRT dataset into 10 folds\n",
    "import torch, glob, os\n",
    "\n",
    "data = torch.load(\".cache/dataset/mmae-dataset/split-1.5mm-segment-only/CMRI.pt\")\n",
    "\n",
    "\n",
    "def is_label(modality: int):\n",
    "    return modality >= 11 and modality <= 20\n",
    "\n",
    "\n",
    "def is_any_label(a: int, b: int, c: int, d: int, e: int):\n",
    "    return is_label(a) or is_label(b) or is_label(c) or is_label(d) or is_label(e)\n",
    "\n",
    "\n",
    "image, pair, image_meta, merged_pair, all_merged_pair = (\n",
    "    data[\"image\"],\n",
    "    data[\"pair\"],\n",
    "    data[\"image_meta\"],\n",
    "    data[\"merged_pair\"],\n",
    "    data[\"all_merged_pair\"],\n",
    ")\n",
    "\n",
    "new_image_index = [[] for _ in range(10)]\n",
    "new_image_mapping = [dict() for _ in range(10)]\n",
    "new_pair_list = [[] for _ in range(10)]\n",
    "new_merge_pair_list = {\n",
    "    \"merged_pair\": [[] for _ in range(10)],\n",
    "    \"all_merged_pair\": [[] for _ in range(10)],\n",
    "}\n",
    "\n",
    "for pair in pair.tolist():\n",
    "    src_modality, src_index, target_modality, target_index = pair\n",
    "    uid = image_meta[src_index][0]\n",
    "    partition = uid % 10\n",
    "    if src_index not in new_image_mapping[partition]:\n",
    "        new_image_mapping[partition][src_index] = len(new_image_index[partition])\n",
    "        new_image_index[partition].append(src_index)\n",
    "    if target_index not in new_image_mapping[partition]:\n",
    "        new_image_mapping[partition][target_index] = len(new_image_index[partition])\n",
    "        new_image_index[partition].append(target_index)\n",
    "\n",
    "    src_index = new_image_mapping[partition][src_index]\n",
    "    target_index = new_image_mapping[partition][target_index]\n",
    "    new_pair_list[partition].append([src_modality, src_index, target_modality, target_index])\n",
    "\n",
    "for merged_pair_key in [\"merged_pair\", \"all_merged_pair\"]:\n",
    "    for merged_pair in data[merged_pair_key].tolist():\n",
    "        src_modality, src_index, attr1, attr2, attr3, attr4, attr5, idx1, idx2, idx3, idx4, idx5 = merged_pair\n",
    "        uid = image_meta[src_index][0]\n",
    "        partition = uid % 10\n",
    "\n",
    "        if not is_any_label(attr1, attr2, attr3, attr4, attr5):\n",
    "            continue\n",
    "        if src_index not in new_image_mapping[partition]:\n",
    "            new_image_mapping[partition][src_index] = len(new_image_index[partition])\n",
    "            new_image_index[partition].append(src_index)\n",
    "        src_index = new_image_mapping[partition][src_index]\n",
    "        if is_label(attr1):\n",
    "            if idx1 not in new_image_mapping[partition]:\n",
    "                new_image_mapping[partition][idx1] = len(new_image_index[partition])\n",
    "                new_image_index[partition].append(idx1)\n",
    "            idx1 = new_image_mapping[partition][idx1]\n",
    "        if is_label(attr2):\n",
    "            if idx2 not in new_image_mapping[partition]:\n",
    "                new_image_mapping[partition][idx2] = len(new_image_index[partition])\n",
    "                new_image_index[partition].append(idx2)\n",
    "            idx2 = new_image_mapping[partition][idx2]\n",
    "        if is_label(attr3):\n",
    "            if idx3 not in new_image_mapping[partition]:\n",
    "                new_image_mapping[partition][idx3] = len(new_image_index[partition])\n",
    "                new_image_index[partition].append(idx3)\n",
    "            idx3 = new_image_mapping[partition][idx3]\n",
    "        if is_label(attr4):\n",
    "            if idx4 not in new_image_mapping[partition]:\n",
    "                new_image_mapping[partition][idx4] = len(new_image_index[partition])\n",
    "                new_image_index[partition].append(idx4)\n",
    "            idx4 = new_image_mapping[partition][idx4]\n",
    "        if is_label(attr5):\n",
    "            if idx5 not in new_image_mapping[partition]:\n",
    "                new_image_mapping[partition][idx5] = len(new_image_index[partition])\n",
    "                new_image_index[partition].append(idx5)\n",
    "            idx5 = new_image_mapping[partition][idx5]\n",
    "        new_merge_pair_list[merged_pair_key][partition].append(\n",
    "            [\n",
    "                src_modality,\n",
    "                src_index,\n",
    "                attr1,\n",
    "                attr2,\n",
    "                attr3,\n",
    "                attr4,\n",
    "                attr5,\n",
    "                idx1,\n",
    "                idx2,\n",
    "                idx3,\n",
    "                idx4,\n",
    "                idx5,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "for partition in range(10):\n",
    "    p_image = image[new_image_index[partition]]\n",
    "    p_image_meta = image_meta[new_image_index[partition]]\n",
    "    p_pair = torch.tensor(new_pair_list[partition])\n",
    "    p_merged_pair = torch.tensor(new_merge_pair_list[\"merged_pair\"][partition])\n",
    "    p_all_merged_pair = torch.tensor(new_merge_pair_list[\"all_merged_pair\"][partition])\n",
    "    torch.save(\n",
    "        {\n",
    "            \"image\": p_image,\n",
    "            \"pair\": p_pair,\n",
    "            \"image_meta\": p_image_meta,\n",
    "            \"merged_pair\": p_merged_pair,\n",
    "            \"all_merged_pair\": p_all_merged_pair,\n",
    "        },\n",
    "        f\".cache/dataset/mmae-dataset/split-1.5mm-segment-only/CMRI-{partition}.pt\",\n",
    "    )\n",
    "    print(f\"partition {partition}, {len(p_image)} images, {len(p_pair)} pairs\")\n",
    "del data, image, pair, image_meta, merged_pair, all_merged_pair\n",
    "del new_image_index, new_image_mapping, new_pair_list, new_merge_pair_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed ACDC.pt...\t 38346 images, 38346 pairs\n",
      "processed CMRI-private.pt...\t 48195 images, 49662 pairs\n",
      "processed CMRI.pt...\t 29450 images, 29450 pairs\n",
      "processed CT_MR_2D_Dataset_DA.pt...\t 1568 images, 1568 pairs\n",
      "processed CoA_MRIData.pt...\t 2641 images, 2641 pairs\n",
      "processed DOI_10.7910_DVN_CI3WB6.pt...\t 42225 images, 42225 pairs\n",
      "processed DOI_10.7910_DVN_JMZHVI.pt...\t 16000 images, 16000 pairs\n",
      "processed DOI_10.7910_DVN_N1R1Q4.pt...\t 1050 images, 1050 pairs\n",
      "processed EMIDEC.pt...\t 1066 images, 1066 pairs\n",
      "processed HVSMR2016.pt...\t 3600 images, 3600 pairs\n",
      "processed LeftAtrialSegmentationChallenge2013.pt...\t 3568 images, 3568 pairs\n",
      "processed LeftAtrialSegmentationKaggle.pt...\t 2271 images, 2271 pairs\n",
      "processed MMWHS2017.pt...\t 23980 images, 23980 pairs\n",
      "processed MyoPS2020.pt...\t 522 images, 1566 pairs\n",
      "processed RegionalMulti-viewLearningForCardiacMotionAnalysis.pt...\t 4101 images, 4101 pairs\n",
      "processed SunnybrookCardiacData.pt...\t 51545 images, 51545 pairs\n",
      "processed VarDA.pt...\t 1077 images, 1077 pairs\n",
      "processed mnms.pt...\t 106159 images, 106159 pairs\n",
      "processed msd-seg.pt...\t 3568 images, 3568 pairs\n",
      "processed yorku_CardiacMRIDataset.pt...\t 7980 images, 7980 pairs\n",
      "total 388912 train images, 391423 pairs!\n"
     ]
    }
   ],
   "source": [
    "# filter image only & pair\n",
    "import torch, glob\n",
    "from torchvision.transforms import functional as F, InterpolationMode\n",
    "\n",
    "os.makedirs(\".cache/dataset/mmae-dataset/split-1.5mm-image-only\", exist_ok=True)\n",
    "\n",
    "file_list = glob.glob(\".cache/dataset/mmae-dataset/split-1.5mm/*.pt\")\n",
    "file_list.sort()\n",
    "\n",
    "total_image_count = 0\n",
    "total_pair_count = 0\n",
    "\n",
    "\n",
    "def is_label(modality: int):\n",
    "    return modality >= 11 and modality <= 20\n",
    "\n",
    "\n",
    "for file in file_list:\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    data = torch.load(file)\n",
    "\n",
    "    new_image_index = []\n",
    "    new_image_mapping = dict()\n",
    "\n",
    "    new_pair_list = []\n",
    "    image, pair = data[\"image\"], data[\"pair\"]\n",
    "    pair_list = pair.tolist()\n",
    "    for pair in pair.tolist():\n",
    "        src_modality, src_index, target_modality, target_index = pair\n",
    "        if is_label(target_modality):\n",
    "            continue\n",
    "\n",
    "        if src_index not in new_image_mapping:\n",
    "            new_image_mapping[src_index] = len(new_image_index)\n",
    "            new_image_index.append(src_index)\n",
    "        if target_index not in new_image_mapping:\n",
    "            new_image_mapping[target_index] = len(new_image_index)\n",
    "            new_image_index.append(target_index)\n",
    "\n",
    "        src_index = new_image_mapping[src_index]\n",
    "        target_index = new_image_mapping[target_index]\n",
    "        new_pair_list.append([src_modality, src_index, target_modality, target_index])\n",
    "    data[\"pair\"] = torch.tensor(new_pair_list)\n",
    "\n",
    "    data[\"image_meta\"] = data[\"image_meta\"][new_image_index]\n",
    "    data[\"image\"] = image[new_image_index]\n",
    "    if len(new_image_index) > 0:\n",
    "        torch.save(\n",
    "            data,\n",
    "            f\".cache/dataset/mmae-dataset/split-1.5mm-image-only/{file_name}\",\n",
    "        )\n",
    "    print(f\"processed {file_name}...\\t {len(new_image_index)} images, {len(new_pair_list)} pairs\")\n",
    "    total_image_count += len(new_image_index)\n",
    "    total_pair_count += len(new_pair_list)\n",
    "\n",
    "print(f\"total {total_image_count} train images, {total_pair_count} pairs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition 0, 3090 images, 3090 pairs\n",
      "partition 1, 2340 images, 2340 pairs\n",
      "partition 2, 3000 images, 3000 pairs\n",
      "partition 3, 3210 images, 3210 pairs\n",
      "partition 4, 2970 images, 2970 pairs\n",
      "partition 5, 2850 images, 2850 pairs\n",
      "partition 6, 2970 images, 2970 pairs\n",
      "partition 7, 3120 images, 3120 pairs\n",
      "partition 8, 3070 images, 3070 pairs\n",
      "partition 9, 2830 images, 2830 pairs\n"
     ]
    }
   ],
   "source": [
    "# spit image only CMRT dataset into 10 folds\n",
    "import torch, glob, os\n",
    "\n",
    "data = torch.load(\".cache/dataset/mmae-dataset/split-1.5mm-image-only/CMRI.pt\")\n",
    "\n",
    "\n",
    "image, pair, image_meta = (\n",
    "    data[\"image\"],\n",
    "    data[\"pair\"],\n",
    "    data[\"image_meta\"],\n",
    ")\n",
    "\n",
    "new_image_index = [[] for _ in range(10)]\n",
    "new_image_mapping = [dict() for _ in range(10)]\n",
    "new_pair_list = [[] for _ in range(10)]\n",
    "\n",
    "for pair in pair.tolist():\n",
    "    src_modality, src_index, target_modality, target_index = pair\n",
    "    uid = image_meta[src_index][0]\n",
    "    partition = uid % 10\n",
    "    if src_index not in new_image_mapping[partition]:\n",
    "        new_image_mapping[partition][src_index] = len(new_image_index[partition])\n",
    "        new_image_index[partition].append(src_index)\n",
    "    if target_index not in new_image_mapping[partition]:\n",
    "        new_image_mapping[partition][target_index] = len(new_image_index[partition])\n",
    "        new_image_index[partition].append(target_index)\n",
    "\n",
    "    src_index = new_image_mapping[partition][src_index]\n",
    "    target_index = new_image_mapping[partition][target_index]\n",
    "    new_pair_list[partition].append([src_modality, src_index, target_modality, target_index])\n",
    "\n",
    "\n",
    "for partition in range(10):\n",
    "    p_image = image[new_image_index[partition]]\n",
    "    p_image_meta = image_meta[new_image_index[partition]]\n",
    "    p_pair = torch.tensor(new_pair_list[partition])\n",
    "    torch.save(\n",
    "        {\n",
    "            \"image\": p_image,\n",
    "            \"pair\": p_pair,\n",
    "            \"image_meta\": p_image_meta,\n",
    "        },\n",
    "        f\".cache/dataset/mmae-dataset/split-1.5mm-image-only/CMRI-{partition}.pt\",\n",
    "    )\n",
    "    print(f\"partition {partition}, {len(p_image)} images, {len(p_pair)} pairs\")\n",
    "del data, image, pair, image_meta\n",
    "del new_image_index, new_image_mapping, new_pair_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/ACDC.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/CMRI-private.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/CT_MR_2D_Dataset_DA.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/CoA_MRIData.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/DOI_10.7910_DVN_CI3WB6.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/DOI_10.7910_DVN_JMZHVI.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/DOI_10.7910_DVN_N1R1Q4.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/EMIDEC.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/HVSMR2016.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/LeftAtrialSegmentationChallenge2013.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/LeftAtrialSegmentationKaggle.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/MMWHS2017.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/MyoPS2020.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/RegionalMulti-viewLearningForCardiacMotionAnalysis.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/SunnybrookCardiacData.pt\n",
      "valid file: .cache/dataset/mmae-dataset/split-1.5mm/VarDA.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/mnms.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/msd-seg.pt\n",
      "train file: .cache/dataset/mmae-dataset/split-1.5mm/yorku_CardiacMRIDataset.pt\n",
      "processed dataset.pt completed! 471332 images, 2391852 train pairs, 100300 valid pairs, 29450 all cmri pairs\n",
      "统计各个target_modality的数量\n",
      "tensor(0, dtype=torch.int32) = 2676\n",
      "tensor(1, dtype=torch.int32) = 215162\n",
      "tensor(2, dtype=torch.int32) = 126000\n",
      "tensor(3, dtype=torch.int32) = 963290\n",
      "tensor(4, dtype=torch.int32) = 149400\n",
      "tensor(5, dtype=torch.int32) = 115710\n",
      "tensor(6, dtype=torch.int32) = 16404\n",
      "tensor(7, dtype=torch.int32) = 16000\n",
      "tensor(11, dtype=torch.int32) = 193840\n",
      "tensor(12, dtype=torch.int32) = 138040\n",
      "tensor(13, dtype=torch.int32) = 213390\n",
      "tensor(14, dtype=torch.int32) = 93660\n",
      "tensor(15, dtype=torch.int32) = 53430\n",
      "tensor(16, dtype=torch.int32) = 42070\n",
      "tensor(17, dtype=torch.int32) = 46440\n",
      "tensor(18, dtype=torch.int32) = 6340\n"
     ]
    }
   ],
   "source": [
    "# make mmae dataset\n",
    "\n",
    "# all image + train pair + valid pair + train_cmri_pair/{i} + valid_cmri_pair/{i}\n",
    "\n",
    "import torch, glob, os\n",
    "\n",
    "\n",
    "exclude_files = set([\"CMRI.pt\", \"token.pt\"])\n",
    "for i in range(10):\n",
    "    exclude_files.add(f\"CMRI-{i}.pt\")\n",
    "\n",
    "valid_files = set([\"VarDA.pt\"])\n",
    "\n",
    "file_list = glob.glob(\".cache/dataset/mmae-dataset/split-1.5mm/*.pt\")\n",
    "file_list.sort()\n",
    "\n",
    "image_list = [\n",
    "    torch.zeros(1, 214, 214, dtype=torch.uint8),  # zero padding\n",
    "]\n",
    "\n",
    "# 各target_modality数量统计\n",
    "#  (0,2676),\n",
    "#  (1,215162),\n",
    "#  (2,1260),\n",
    "#  (3,96329),\n",
    "#  (4,1494),\n",
    "#  (5,11571),\n",
    "#  (6,16404),\n",
    "#  (7,16000),\n",
    "#  (11,19384),\n",
    "#  (12,13804),\n",
    "#  (13,21339),\n",
    "#  (14,9366),\n",
    "#  (15,5343),\n",
    "#  (16,4207),\n",
    "#  (17,4644),\n",
    "#  (18,634),\n",
    "\n",
    "# 用于平衡各个target_modality的数量\n",
    "repeat_count = dict(\n",
    "    [\n",
    "        (0, 1),\n",
    "        (1, 1),  # T1\n",
    "        (2, 100),  # T2\n",
    "        (3, 10),  # CINE\n",
    "        (4, 100),  # LGE\n",
    "        (5, 10),  # SSFP\n",
    "        (6, 1),  # CT\n",
    "        (7, 1),  # Flair\n",
    "        (9, 1),  # Ultrasound\n",
    "        (11, 10),  # LV\n",
    "        (12, 10),  # RV\n",
    "        (13, 10),  # MYO\n",
    "        (14, 10),  # LA\n",
    "        (15, 10),  # RA\n",
    "        (16, 10),  # AO\n",
    "        (17, 10),  # PA\n",
    "        (18, 10),  # VA\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_pair_list = []\n",
    "valid_pair_list = []\n",
    "all_cmri_pair_list = []\n",
    "train_cmri_list = [[] for _ in range(10)]\n",
    "valid_cmri_list = [[] for _ in range(10)]\n",
    "\n",
    "index = 1\n",
    "for file in file_list:\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    if file_name in exclude_files:\n",
    "        continue\n",
    "\n",
    "    is_valid = file_name in valid_files\n",
    "\n",
    "    data = torch.load(file)\n",
    "    image, pair = data[\"image\"], data[\"pair\"]\n",
    "    pair[:, 1] += index\n",
    "    pair[:, 3] += index\n",
    "\n",
    "    # 用于平衡各个target_modality的数量\n",
    "    pair_index = []\n",
    "    for i, modality in enumerate(pair[:, 2].tolist()):\n",
    "        pair_index.extend([i] * repeat_count[modality])\n",
    "    pair = pair[pair_index]\n",
    "\n",
    "    image_list.append(image)\n",
    "    index += image.shape[0]\n",
    "    if is_valid:\n",
    "        print(f\"valid file: {file}\")\n",
    "        valid_pair_list.append(pair)\n",
    "    else:\n",
    "        print(f\"train file: {file}\")\n",
    "        train_pair_list.append(pair)\n",
    "\n",
    "\n",
    "# 划分数据集, 做交叉测试\n",
    "data = torch.load(\".cache/dataset/mmae-dataset/split-1.5mm/CMRI.pt\")\n",
    "image, image_meta, pair = data[\"image\"], data[\"image_meta\"], data[\"pair\"]\n",
    "image_meta = image_meta.tolist()\n",
    "for pair in pair.tolist():\n",
    "    src_modality, src_index, target_modality, target_index = pair\n",
    "    uid = image_meta[src_index][0]\n",
    "    partition = uid % 10\n",
    "    src_index += index\n",
    "    target_index += index\n",
    "    if target_modality != 20:  # 因为训练集没有心包脂肪, 所以验证集也不需要验证脂肪\n",
    "        all_cmri_pair_list.append([src_modality, src_index, target_modality, target_index])\n",
    "    for i in range(10):\n",
    "        if i == partition:\n",
    "            valid_cmri_list[i].append([src_modality, src_index, target_modality, target_index])\n",
    "        else:\n",
    "            train_cmri_list[i].append([src_modality, src_index, target_modality, target_index])\n",
    "image_list.append(image)\n",
    "index += image.shape[0]\n",
    "\n",
    "image_list = torch.cat(image_list, dim=0).to(torch.uint8)\n",
    "train_pair_list = torch.cat(train_pair_list, 0).int()\n",
    "valid_pair_list = torch.cat(valid_pair_list, 0).int()\n",
    "all_cmri_pair_list = torch.tensor(all_cmri_pair_list).int()\n",
    "train_cmri_list = [torch.tensor(train_cmri_list[i]).int() for i in range(10)]\n",
    "valid_cmri_list = [torch.tensor(valid_cmri_list[i]).int() for i in range(10)]\n",
    "\n",
    "assert len(image_list.shape) == 3\n",
    "assert len(train_pair_list.shape) == 2 and train_pair_list.shape[1] == 4\n",
    "assert len(valid_pair_list.shape) == 2 and valid_pair_list.shape[1] == 4\n",
    "assert len(all_cmri_pair_list.shape) == 2 and all_cmri_pair_list.shape[1] == 4\n",
    "assert len(train_cmri_list[0].shape) == 2 and train_cmri_list[0].shape[1] == 4\n",
    "\n",
    "data = {\n",
    "    \"image\": image_list,  # (bs, h, w)\n",
    "    \"train_pair\": train_pair_list,  # (bs, [src_modality, src_index, target_modality, target_index])\n",
    "    \"valid_pair\": valid_pair_list,  # (bs, [src_modality, src_index, target_modality, target_index])\n",
    "    \"all_cmri_pair\": all_cmri_pair_list,  # (bs, [src_modality, src_index, target_modality, target_index])\n",
    "}\n",
    "for i in range(10):\n",
    "    # (bs, [src_modality, src_index, target_modality, target_index])\n",
    "    # finetune的时候使用, 10 folds\n",
    "    data[f\"train_cmri_pair/{i}\"] = train_cmri_list[i]\n",
    "    data[f\"valid_cmri_pair/{i}\"] = valid_cmri_list[i]\n",
    "\n",
    "torch.save(\n",
    "    data,\n",
    "    f\".cache/dataset/mmae-dataset/dataset.pt\",\n",
    ")\n",
    "print(\n",
    "    f\"processed dataset.pt completed! {image_list.shape[0]} images, {train_pair_list.shape[0]} train pairs, {valid_pair_list.shape[0]} valid pairs, {all_cmri_pair_list.shape[0]} all cmri pairs\"\n",
    ")\n",
    "\n",
    "print(\"统计各个target_modality的数量\")\n",
    "modalities = train_pair_list[:, 2].unique()\n",
    "for mod in modalities:\n",
    "    print(mod, \"=\", (train_pair_list[:, 2] == mod).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "file = \".cache/dataset/mmae-dataset/dataset.pt\"\n",
    "data = torch.load(file)\n",
    "(\n",
    "    all_image,  # (bs, h, w)\n",
    "    train_pair,  # (bs, [src_modality, src_index, target_modality, target_index])\n",
    "    valid_pair,\n",
    "    all_cmri_pair,  # (bs, [src_modality, src_index, target_modality, target_index])\n",
    "    train_cmri_pair,\n",
    "    valid_cmri_pair,\n",
    ") = (\n",
    "    data[\"image\"],\n",
    "    data[\"train_pair\"],\n",
    "    data[\"valid_pair\"],\n",
    "    data[\"all_cmri_pair\"],\n",
    "    data[\"train_cmri_pair/0\"],\n",
    "    data[\"valid_cmri_pair/0\"],\n",
    ")\n",
    "\n",
    "\n",
    "def draw_image_simple(pair, title=\"\"):\n",
    "    pair_count = pair.shape[0] - 1\n",
    "\n",
    "    @interact\n",
    "    def _draw_image(pair_index=(0, pair_count)):\n",
    "        src_modality, src_index, target_modality, target_index = pair[pair_index].tolist()\n",
    "\n",
    "        image = all_image[[src_index, target_index], ::]\n",
    "\n",
    "        image = image.permute(1, 0, 2).reshape(214, -1)\n",
    "\n",
    "        plt.title([title, src_modality, src_index, target_modality, target_index])\n",
    "        plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "    return _draw_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d00474be7e64ff29eae91eb716ebb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=219808, description='pair_index', max=439616), Output()), _dom_classes=(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.draw_image_simple.<locals>._draw_image(pair_index=(0, 439616))>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_image_simple(train_pair, \"train_pair\")\n",
    "# draw_image_simple(valid_pair, \"valid_pair\")\n",
    "# draw_image_simple(all_cmri_pair, \"all_cmri_pair\")\n",
    "# draw_image_simple(train_cmri_pair, \"train_cmri_pair\")\n",
    "# draw_image_simple(valid_cmri_pair, \"valid_cmri_pair\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
